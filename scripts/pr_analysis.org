#+TITLE: Pull request analysis
#+AUTHOR: Ond≈ôej Kuhejda
#+PROPERTY: header-args+ :comments both
#+PROPERTY: header-args+ :tangle "analysis.R"
* Packages
  Following packages are needed for data manipulation:
  #+BEGIN_SRC R
    # install.packages("tidyverse")
    library("dplyr")
    library("purrr")
  #+END_SRC

  This package is needed for PCA plotting:
  #+BEGIN_SRC R
    # install.packages("ggfortify")
    library("ggfortify")
  #+END_SRC

  Following package is used for printing tables into the PDF file:
  #+BEGIN_SRC R
    # install.packages("gridExtra")
    library("gridExtra")
  #+END_SRC

  This library is used to format percentages:
  #+BEGIN_SRC R
    # install.packages("scales")
    library("scales")
  #+END_SRC

  This package can be used to export tables into the LaTeX:
  #+BEGIN_SRC R
    # install.packages("xtable")
    library("xtable")
  #+END_SRC

  Following package is able to export plots into the LaTeX (TikZ):
  #+BEGIN_SRC R
    # install.packages("tikzDevice")
    library("tikzDevice")
  #+END_SRC

  This library is used to calculate effect size (Cramer's V):
  #+BEGIN_SRC R
    install.packages("lsr")
    library("lsr")
  #+END_SRC
* Data preprocessing
** Dataset loading
   #+BEGIN_SRC R
     prs <- read.csv("data.csv", header=TRUE, sep = ",", check.names=FALSE)

     dim(prs)

     str(prs)

     summary(prs)
   #+END_SRC
** Convert strings to booleans
   #+BEGIN_SRC R
     prs$accepted <- prs$accepted == "True"
     prs$submitter_is_project_member <- prs$submitter_is_project_member == "True"
   #+END_SRC
** Filter linter results
   #+BEGIN_SRC R
     prsQuality <- prs %>% select("accepted" | starts_with('results_'))

     summary(prsQuality)
   #+END_SRC
* Research Question 1
  Print the table with a summary of all projects:
  #+BEGIN_SRC R
    projects_summary <- (prs %>%
                         mutate(introduced=rowSums(mutate_all(select(., starts_with("results_")), ~if_else(.x < 0, 0L, .x))),
                                fixed=rowSums(mutate_all(select(., starts_with("results_")), ~if_else(.x > 0, 0L, -.x))),
                                rejected=!accepted) %>% group_by(project_name) %>% rename(Project=project_name) %>%
                         summarise(Stars=first(project_number_of_watchers), "Analyzed PRs"=n(),
                                   Accepted=percent(sum(accepted)/n()), Rejected=percent(sum(rejected)/n()),
                                   "Introduced issues"=mean(introduced, trim=0.05),
                                   "Fixed issues"=mean(fixed, trim=0.05)) %>% arrange(desc(Stars))
                         )

    print(xtable(projects_summary, type="latex", align=c("l", "|p{3.5cm}", "p{1.5cm}", "p{1.5cm}", "p{1.5cm}",
                                                        "p{1.5cm}", "p{1.5cm}", "p{1cm}|")),
          file="projects_summary.tex", include.rownames=FALSE,
          add.to.row=list(pos=list(0), command=c("\\hline")), floating=FALSE)
  #+END_SRC

  Print the scatter plot between number of stars and percentage of accepted PRs:
  #+BEGIN_SRC R
    tikz(filename="stars_and_acceptance.tex", width=10, height=6)
    ggplot(projects_summary %>% mutate_at("Accepted", function(x) readr::parse_number(x)), aes(x=Stars, y=Accepted)) + geom_() +
        geom_smooth(method='lm') + labs(y="Accepted pull requests (\\%)")
    dev.off()
  #+END_SRC

  Print the heat map of fixed and introduced issues (PR quality overview):
  #+BEGIN_SRC R
    tikz(filename="pr_quality_heat_map.tex", width=10, height=10)
    ggplot(prs %>% mutate(introduced=rowSums(mutate_all(select(., starts_with("results_")), ~if_else(.x < 0, 0L, .x))),
                          fixed=rowSums(mutate_all(select(., starts_with("results_")), ~if_else(.x > 0, 0L, -.x)))),
           aes(x=introduced, y=fixed)) + xlim(-11,210) + ylim(-11,210) + geom_bin2d(binwidth=10) +
        scale_fill_gradient(trans="log10") + labs(x="Introduced issues", y="Fixed issues",
                                                  fill="Number of\npull requests")
    dev.off()
  #+END_SRC

  Print the summary about all pull request of the given language:
  #+BEGIN_SRC R
    prs %>% mutate(introduced=rowSums(mutate_all(select(., starts_with("results_")), ~if_else(.x < 0, 0L, .x))),
                   fixed=rowSums(mutate_all(select(., starts_with("results_")), ~if_else(.x > 0, 0L, -.x))),
                   rejected=!accepted) %>%
        summarise(Stars=mean(project_number_of_watchers), "Analyzed PRs"=n(),
                  Accepted=percent(sum(accepted)/n()), Rejected=percent(sum(rejected)/n()),
                  "Introduced issues"=mean(introduced, trim=0.05), "Fixed issues"=mean(fixed, trim=0.05))
  #+END_SRC

  Do the same but group by acceptance:
  #+BEGIN_SRC R
    prs %>% mutate(introduced=rowSums(mutate_all(select(., starts_with("results_")), ~if_else(.x < 0, 0L, .x))),
                   fixed=rowSums(mutate_all(select(., starts_with("results_")), ~if_else(.x > 0, 0L, -.x))),
                   rejected=!accepted) %>% group_by(accepted) %>%
        summarise(Stars=mean(project_number_of_watchers), "Analyzed PRs"=n(),
                  Accepted=percent(sum(accepted)/n()), Rejected=percent(sum(rejected)/n()),
                  "Introduced issues"=mean(introduced, trim=0.05), "Fixed issues"=mean(fixed, trim=0.05)) %>%
        print(width = Inf)
  #+END_SRC

  *TODO*: add information about number of projects in which issue occurred
  Summarize information about individual issues (compute maximum, minimum etc.):
  #+BEGIN_SRC R
    (issues <- prsQuality %>% group_by(accepted) %>% summarise(across(everything(),
                                                                      tibble::lst(max, min, mean, introduced_by=~sum(. > 0),
                                                                                  fixed_by=~sum(. < 0), appeared_in=~sum(. != 0)),
                                                                     .names="{.col}***{.fn}")) %>%
            tidyr::pivot_longer(cols=starts_with("results_"), names_to=c("issue", ".value"), names_sep="\\*\\*\\*") %>%
            group_by(accepted) %>% group_split() %>% bind_cols() %>% select(2:8, 11:16) %>%
            rename_with(.cols=2:7, .fn=function(x) sub("^", "rejected.", sub("\\..*", "", x))) %>%
            rename_with(.cols=8:13, .fn=function(x) sub("^", "accepted.", sub("\\..*", "", x))) %>%
            rename(issue = issue...2) %>% mutate_at("issue", function(x) sub("results_([^_]+)_", "", x)) %>%
            tidyr::extract(issue, into=c("type", "issue"), "^([^_]+)_(.*)"))
  #+END_SRC

  Print the table with summary into the PDF file:
  #+BEGIN_SRC R
    pdf("issues.pdf", height=75, width=25)
    grid.table(issues)
    dev.off()
  #+END_SRC

  Print the number of different issue that was detected in the PRs:
  #+BEGIN_SRC R
    nrow(issues)
  #+END_SRC

  Summarize the issue categories:
  #+BEGIN_SRC R
    issueTypesSummary <- tibble(
      type = character(),
      introduced_total = integer(),
      introduced_by = integer(),
      fixed_total = integer(),
      fixed_by = integer()
    )
    for (type in unique(issues$type)) {
        issueTypesSummary <- issueTypesSummary %>%
            bind_rows(prs %>% select(starts_with("results_") & contains(type)) %>%
                      mutate(introduced=rowSums(mutate_all(., ~if_else(.x < 0, 0L, .x))),
                             fixed=rowSums(mutate_all(., ~if_else(.x > 0, 0L, -.x)))) %>%
                      summarize(type=type, introduced_total=sum(introduced), introduced_by=sum(introduced > 0),
                                fixed_total=sum(fixed), fixed_by=sum(fixed > 0)))
    }

    print(xtable((issueTypesSummary %>% rename(Category=type, "Introduced in total"=introduced_total,
                                               "#PRs which introduced"=introduced_by, "Fixed in total"=fixed_total,
                                               "#PRs which fixed"=fixed_by)),
                 type="latex", align=c("l", "|p{2cm}", "p{2cm}", "p{2cm}", "p{2cm}", "p{2cm}|"), digits=c(0,0,0,0,0,0)),
          file="issue_types_summary.tex", include.rownames=FALSE,
          add.to.row=list(pos=list(0), command=c("\\hline")), floating=FALSE)
  #+END_SRC

  Create a barplot with issues and their average counts in accepted/rejected pull requests:
  #+BEGIN_SRC R
    barplot(t(as.matrix(issues %>% select(accepted.mean, rejected.mean))), beside=TRUE, legend.text=TRUE,
            xlab="issue", ylab="on average in one PR")
  #+END_SRC

  List the issues sorted by the number of accepted pull request which introduced them:
  #+BEGIN_SRC R
    issues %>% arrange(desc(accepted.introduced_by)) %>% select(type, issue, accepted.introduced_by)
  #+END_SRC

  List the issues sorted by the number of rejected pull request which introduced them:
  #+BEGIN_SRC R
    issues %>% arrange(desc(rejected.introduced_by)) %>% select(type, issue, rejected.introduced_by)
  #+END_SRC

  List the issues sorted by the number of accepted pull request which fixed them:
  #+BEGIN_SRC R
    issues %>% arrange(desc(accepted.fixed_by)) %>% select(type, issue, accepted.fixed_by)
  #+END_SRC

  List the issues sorted by the number of rejected pull request which fixed them:
  #+BEGIN_SRC R
    issues %>% arrange(desc(rejected.fixed_by)) %>% select(type, issue, rejected.fixed_by)
  #+END_SRC

  List the issues and the percentage in how many pull requests they change the quality:
  #+BEGIN_SRC R
    issues %>% transmute(type, issue, appeared_in=(rejected.appeared_in + accepted.appeared_in)) %>%
        arrange(desc(appeared_in)) %>% mutate(percent_of_prs=percent(appeared_in/nrow(prs))) %>%
        print(n=Inf)
  #+END_SRC

  Print the issues that were fixed in the larger number of PRs then introduced.
  #+BEGIN_SRC R
    issues %>% transmute(type, issue, fixed_more_times=(accepted.fixed_by + rejected.fixed_by -
                                                        accepted.introduced_by - rejected.introduced_by)) %>%
        arrange(desc(fixed_more_times)) %>% print(n=Inf)
  #+END_SRC

  Create a barplot with issues on the x-axis and number of pull request in which the issues were fixed/introduced on the y-axis:
  #+BEGIN_SRC R
    tikz(filename="issues_appeared_in.tex", width=7, height=3)
    issues %>% transmute(type, appeared_in=100*(rejected.appeared_in + accepted.appeared_in)/nrow(prs)) %>%
        arrange(desc(appeared_in)) %>% mutate(pos=1:n()) %>%
        ggplot(aes(x=pos, y=appeared_in, fill=type)) + geom_col() + labs(x="Issues", y="Pull Requests (\\%)", fill="Types") +
        theme(axis.ticks.x=element_blank(), axis.text.x=element_blank())
    dev.off()
  #+END_SRC

  Create a scatter plot with issue types on the x-axis and number of pull request in which the issues were
  fixed/introduced on the y-axis:
  #+BEGIN_SRC R
    tikz(filename="issues_types_and_prs.tex", width=7, height=5)
    issues %>% transmute(type, appeared_in=100*(rejected.appeared_in + accepted.appeared_in)/nrow(prs)) %>%
        ggplot(aes(x=reorder(type, desc(appeared_in), mean), y=appeared_in)) + labs(x="Issue types (sorted by y-axis mean)",
                                                                                    y="Pull Requests (\\%)") + geom_point()
    dev.off()
  #+END_SRC
* Research Question 2
  *TODO:* extract this into the function and do it for =classification= and =classification_introduced=
  Import the issue importance from CSV files:
  #+BEGIN_SRC R
    path <- "classification/Importances_drop/values/"
    issueImportance <- (lapply(list.files(path=path, pattern="*.csv"),
                               (function (file) read.csv(paste(path, file, sep=""), header=TRUE, sep = ",",
                                                         check.names=FALSE) %>% rename_with(~sub("_ruleid.csv", "", file),
                                                                                            Importance)))
                       ) %>% reduce(full_join, by="Variables") %>%
                             mutate_at("Variables", function(x) sub("results_([^_]+)_", "", x)) %>%
                             tidyr::extract(Variables, into=c("type", "issue"), "^([^_]+)_(.*)")
  #+END_SRC

  Sort issues by their average importance and print them:
  #+BEGIN_SRC R
    issueImportance %>% mutate(mean=rowMeans(.[,-1:-2])) %>% arrange(desc(mean)) %>% head(10) %>% print
  #+END_SRC

  Sort issues by their average importance and plot them in the barplot:
  #+BEGIN_SRC R
    tikz(filename="issue_importance.tex", width=4.5, height=3)
    issueImportance %>% mutate(mean=rowMeans(.[,-1:-2])) %>% arrange(desc(mean)) %>% head(10) %>%
        tidyr::gather(classifier, importance, -c(type, issue, mean)) %>% ggplot() +
            geom_bar(aes(x=reorder(issue, mean), y=(100 * importance), fill=classifier), stat='identity') + coord_flip() +
            labs(x="Issues", y="Importance (\\%)", fill="Classifier") + geom_hline(yintercept=0)
    dev.off()
  #+END_SRC
* Research Question 3
** PCA scatterplot
   #+BEGIN_SRC R
     set.seed(135089)
     prsSample <- prsQuality %>% sample_n(2000, replace=FALSE)

     acceptancePCA <- prcomp(prsSample %>% select(-accepted))

     tikz(filename="acceptance_pca.tex", width=6, height=4)
     pallete = c("red", "green")
     (autoplot(acceptancePCA, data=prsSample, colour="accepted") + scale_colour_manual(values=pallete) +
                                             xlim(-0.0025, 0.0025) + ylim(-0.01, 0.01) +
      labs(colour="Accepted",
                                                       x=paste("PC1 (", summary(acceptancePCA)$importance[2,1] * 100, "\\%)", sep=""),
                                                       y=paste("PC2 (", summary(acceptancePCA)$importance[2,2] * 100, "\\%)", sep="")
                                                       )
     )
     dev.off()
   #+END_SRC
** Contingency matrices
   Define function that will be used to plot results of chi-square test of independence:
   #+BEGIN_SRC R
     chsqt_plot <- function(chsqt) {
         ggplot(data=data.frame(Frequency=c(chsqt$observed[1,1], chsqt$observed[1,2], chsqt$observed[2,1], chsqt$observed[2,2],
                                            chsqt$expected[1,1], chsqt$expected[1,2], chsqt$expected[2,1], chsqt$expected[2,2]),
                                Value=rep(c("Observed", "Expected"), each=4),
                                Quality=rep(c("Issue detected", "Without an issue"), times=4),
                                Acceptance=rep(rep(c("Rejected pull requests", "Accepted pull requests"), each=2), times=2)
                                ), aes(x=Quality, y=Frequency, fill=Value)) + geom_bar(stat="identity", position="dodge") +
             facet_grid(~ Acceptance) + labs(x="Presence of some quality issue", y="Pull request frequency")
     }
   #+END_SRC

   Does an introduction of some code quality issue in the PR affects its acceptance?
   #+BEGIN_SRC R
     qualityCT <- data.frame((prsQuality %>% transmute(accepted, issueTypes=rowSums(.[-1]>0)) %>% group_by(accepted) %>%
                              summarize(across(everything(), tibble::lst(introduced=~sum(.>0), didNotIntroduced=~sum(.==0)))))[,-1])
     colnames(qualityCT) <- c("Issue introduced", "Issue not introduced")
     rownames(qualityCT) <- c("Rejected", "Accepted")

     chsqt <- chisq.test(qualityCT)

     chsqt

     chsqt$observed

     round(chsqt$expected, 2)

     cramersV(qualityCT)

     tikz(filename="acceptance_ct.tex", width=10, height=6)
     chsqt_plot(chsqt)
     dev.off()
   #+END_SRC

   Filter PRs that only modified some source code files and test them:
   #+BEGIN_SRC R
     qualityModCT <- data.frame((prs %>% filter(modified == linted_and_modified, added == 0, deleted == 0) %>%
                                 select("accepted" | starts_with("results_")) %>%
                                 transmute(accepted, issueTypes=rowSums(.[-1]>0)) %>% group_by(accepted) %>%
                                 summarize(across(everything(), tibble::lst(introduced=~sum(.>0),
                                                                            didNotIntroduced=~sum(.==0)))))[,-1]
                                )
     colnames(qualityModCT) <- c("Issue introduced", "Issue not introduced")
     rownames(qualityModCT) <- c("Rejected", "Accepted")

     chsqtMod <- chisq.test(qualityModCT)

     chsqtMod

     chsqtMod$observed

     round(chsqtMod$expected, 2)

     cramersV(qualityModCT)

     tikz(filename="acceptance_mod_ct.tex", width=10, height=6)
     chsqt_plot(chsqtMod)
     dev.off()
   #+END_SRC

   *TODO*: Test each project independently.

   *TODO* Does a fixing of some code quality issue in the PR affects its acceptance?
   #+BEGIN_SRC R
     fixedCont <- data.frame((prsQuality %>% transmute(accepted, issueTypes=rowSums(.[-1]<0)) %>% group_by(accepted) %>%
                    summarize(across(everything(), tibble::lst(fixed=~sum(.>0), didNotFixed=~sum(.==0)))))[,-1])
     rownames(fixedCont) <- c("rejected", "accepted")

     fixedContChisq <- chisq.test(fixedCont)

     fixedContChisq
     fixedContChisq$observed
     round(fixedContChisq$expected, 2)
   #+END_SRC
** ROC curves and AUCs
   *TODO:* extract this into the function and do it for =classification= and =classification_introduced=
   Retrieve classifiers metrics:
   #+BEGIN_SRC R
     path <- "classification_introduced/Metrics/"
     files <- list.files(path=path, pattern="*.csv")
     classificationMetrics <- lapply(files, (function (file) read.csv(paste(path, file, sep=""), header=TRUE, sep = ",",
                                                                      check.names=FALSE)))
     names(classificationMetrics) = lapply(files, function (file) sub("_ruleid.csv", "", file))
     classificationMetrics <- bind_rows(classificationMetrics, .id="Classifier")

     classificationMetrics
   #+END_SRC

   Plot ROC curves:
   #+BEGIN_SRC R
     path <- "classification_introduced/AUCs/values/"
     files <- list.files(path=path, pattern="*.csv")
     classificationROCs <- lapply(files, (function (file) read.csv(paste(path, file, sep=""), header=TRUE, sep = ",", check.names=FALSE)))
     names(classificationROCs) = (classificationMetrics %>% transmute(names=stringr::str_c(Classifier, " (AUC=",
                                                                                           percent(AUC_mean/100), ")"))
                                 )$names

     ggplot(bind_rows(classificationROCs, .id="Classifier"), aes(x=mean_fpr, y=mean_tpr, colour=Classifier)) + geom_line() +
         geom_abline(intercept=0, slope=1, linetype="dashed") + xlab("False Positive Rate (1 - specificity)") +
         ylab("True Positive Rate (sensitivity)")
   #+END_SRC
* Research Question 4
* Code quality and time to close a pull request
** ROC curves and AUCs
   /Analyzed using the Python script./
* Research Question 5
