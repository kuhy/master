#+TITLE: Source Code Quality Impact @@latex:\\@@ on Pull Requests Acceptance
#+AUTHOR: Ond≈ôej Kuhejda
#+OPTIONS: num:2 ':t
#+LANGUAGE: en
* Introduction
  Does code quality influence the acceptance of pull requests?  Although many
  project maintainers view code quality as the most important factor
  regarding pull request acceptance[[cite:integrator]], a recent
  study shows that the presence of quality flaws in the code does not
  influence the acceptance or rejection of pull requests[[cite:quality]].

  To the best of my knowledge, that is the only study that investigated if
  quality issues affect the acceptance of pull requests. Lenarduzzi et al.
  analyzed 28 well-known Java projects and applied several statistical
  techniques to find the relation between code quality and pull request
  acceptance.  The quality of the pull requests was evaluated using the
  open-source tool called PMD. This tool is able to perform static analysis of the
  code and detect common programming flaws, such as code-smells, anti-patterns,
  or code-style violations. Traditional statistical techniques did not find
  any connection between the code quality and pull request acceptance.
  Because of that, they trained several machine learning models to predict the
  acceptance based on the quality issues found in the code. However,
  machine learning models yielded similar results as traditional techniques.

  Unfortunately, the study performed by Lenarduzzi et al. analyzed only projects
  written in Java. Moreover, 22 out of 28 analyzed projects were from the Apache
  Software Foundation. This is the major threat to the generalizability of their
  findings. To address these shortcomings, I analyzed one hundred projects from
  five different programming languages --- Python, Java, Kotlin, Haskell, and
  C/C++ (twenty projects per language). For each programming language, a different
  tool for static analysis was used to evaluate the code quality.
  I applied similar techniques as Lenarduzzi et al. to analyze the relationship between the
  code quality and pull request acceptance. Furthermore, the regression models were
  created to predict the time required to close a pull request using the code quality.
  Subsequently, the retrieved results were compared between individual languages.
  Especially, the questions I am trying to answer are the following:
  - RQ_1 :: Which code issues are typically introduced by the pull requests?
  - RQ_2 :: Are there some particular quality flaws that affect the acceptance of the pull request?
  - RQ_3 :: Is there a relationship between the source code quality and the pull request acceptance?
  - RQ_4 :: Does code quality influence the time required to close a pull request?
  - RQ_5 :: Is code quality impact higher in projects that are using some particular programming language?

  This thesis first discusses code quality and its link to the pull-based
  development model.  After that, the various factors that influence the pull
  request acceptance are mentioned. The next part of the thesis is dedicated to the
  research design. At first, I state how the data about the project was
  retrieved and which methods were used to evaluate the code quality of
  individual pull requests. Then, the used statistical methods are
  introduced. Furthermore, the retrieved results are evaluated separately for
  each programming language. Towards the end, results are compared between
  languages, and potential threats to validity are discussed.  Finally, I
  summarize my findings and compare them to the findings of Lenarduzzi et al.
  Moreover, the potential extensions of my work are proposed.
* Code quality in pull-based development
  The pull-based development model created novel ways how developers can
  interact with each other. Instead of pushing code changes (patches) into
  one central repository, developers can work in a more decentralized and
  distributed way. This is mainly done by using distributed version control
  systems such as Git. Git enables developers to clone repositories and thus
  work independently on projects. Furthermore, Git's branching model helps
  developers to keep track of repository changes and helps to handle the
  conflicts between the different changes of the same code base[[cite:exploratory]].

  To furthermore ease the complicated process of resolving conflicts between
  different changes (of the same code base) and to provide a more user-friendly
  environment for developers, platforms such as GitHub were created. These
  platforms add new ways how the developers can interact beyond the basic
  functionality of Git:
  - The forks enable the creation of the server-side copy of the repository.
  - Pull requests[fn::pull request is commonly abbreviated as PR]
    (on some platforms called merge requests) enables to merge code directly on the platform.
  - Users can report issues found in the projects; therefore, the platform can also serve as a bug-tracking system.
  - The comments can be added to the pull requests and issues in order to build up social interaction between developers.
  - Users can star projects and follow other users, projects, pull requests, or issues.

  In this study, I choose to use GitHub as the main source for data
  mining. GitHub is one of the leading platforms that enables pull-based
  collaboration between developers. GitHub hosts a huge amount of publicly
  available repositories and GitHub also provides public REST API that can be
  easily leveraged for data mining.

  The aim of this thesis is to obtain a large amount of data about GitHub projects
  and analyze the pull request in regard to their code quality. How the code
  quality can be analyzed and how the GitHub platforms contribute to the quality of
  the code itself is discussed in the following chapters.
** Measuring code quality
   Code quality is a very important aspect of every program --- software with high
   code quality has a competitive advantage, is more stable, and is also more
   maintainable than software that is poorly written.

   To be able to evaluate the software in regard to its quality, there needs to
   be some way how the code quality can measured. The testing can be used exactly
   for this purpose --- as a tool for measuring the quality of the source code.
   There are multiple ways how can be testing performed. Testing techniques can
   be divided into two categories: static and dynamic testing techniques[[cite:istqb]].

   In order to use dynamic testing techniques on a large number of programs, there
   are two large obstacles --- the program needs to be executed, and there need
   to be some inputs (with expected outputs) that can then be used for testing.
   Program execution can be problematic. Some programs need to be compiled
   before they can be executed; others require a special environment for their
   execution (specific hardware, operating system, or shared libraries required
   by the program). Moreover, most of the programs do not have predefined sets of
   input that can be used for testing. There exist some techniques that can also be
   used without the predefined inputs, such as fuzzing, but these techniques
   are usually time-consuming. Because of that, dynamic testing techniques are
   not a viable option when dealing with a large number of programs.

   On the other hand, static testing methods suit the analysis of a large
   number of programs better. Static techniques encompass the usage of formal and
   informal reviews, walkthroughs, and inspections; however, these techniques are
   performed by humans and therefore are not viable for large datasets. Because
   of that, in this thesis, the quality of the given source code is evaluated
   using the tools for automatic static analysis (called linters). Linters are
   used to find defects and code smells in the source code without the need for
   the source code's execution.

   The ISO/IEC 25010[[cite:ISO25010]] defines several quality characteristics which can be identified in the software.
   I will now discuss these characteristics in the context of static analysis:
   - Performance efficiency :: evaluates if the application is using the optimal amount of resources.
     The static analysis can help to create a faster code.  For instance, some
     linters are able to detect constructs/functions that are ineffective and
     consume more resources than required.
   - Usability :: is the degree to which the software is easy to use. This quality is often evaluated through
     /usability testing/. On the other hand, there are some properties that can be checked via static analysis,
     such as proper documentation of public interfaces, which contributes to the application's learnability.
   - Reliability :: defines how stable and fault-tolerant the software is.
     Static analysis can unravel error-prone constructs and multi-threading issues
     (that negatively influence stability) and ensure that exception handling is properly implemented.
   - Security :: is concerned with the confidentiality, integrity, and authenticity of the software.
     Linters can detect several security-related issues in the source code, such as the use of vulnerable functions
     or use of the hard-coded values for cryptographic operations.
   - Maintainability :: is the ease with which can be application modified.
     Static analysis can help to ensure that source code is clean and
     understandable.  Source code can be checked if it follows the conventions of
     the given programming language. For instance, Python has an official style
     guide for Python code --- PEP 8[fn::https://www.python.org/dev/peps/pep-0008/].
     This guide defines the conventions that should be followed, such as proper
     indentation of the code blocks, maximum line length, or naming conventions.
     Furthermore, code can be analyzed if the software is properly designed and
     does not use complicated constructs; for instance linter can detect if some
     part of the code is redundant, complicated, or too coupled.
   - Portability :: is the ability to execute software on multiple platforms.
     Some linters are capable of detecting functions and data types that are not portable.

   However, it is important to note that not all linters have the same
   capabilities. Issues that can be detected by the given linter heavily
   depend on the used programming language (some quality issues are
   language-specific). Which linters were used for the purposes of this thesis
   is discussed later in the text.

   The code issues (the number of their occurrences) identified by linters were
   used as a metric to evaluate the code quality of the given pull request.  The
   same approach was used by Lenarduzzi et al.[[cite:quality]] during the evaluation
   of the pull requests code quality.
** GitHub and code quality
   GitHub brings many features that may potentially improve code quality.
   GitHub has a built-in bug tracker which can be used to report issues found in the code.
   Because the issues can be reported by users outside of the core development team,
   the code quality issues can be detected earlier and more efficiently. Bug trackers
   also enable prioritization of issues which helps to decide which problems need attention first.

   Moreover, GitHub enables the creation of pull requests --- a mechanism by which the developers can propose changes to
   the code base. When the pull request is submitted, the maintainers of the repository decide if the changes
   will be applied (merged) or not (rejected). Quality can be one factor that can influence this decision.
   The versatility of Git enables pull requests to be merged in various ways[[cite:exploratory]]: through GitHub facilities,
   using Git merge, or by committing the patch.

   One of the pull requests advantages is the integration with the code review functionality. Maintainers of the projects
   can review the code to improve internal code quality and maintainability.

   GitHub provides CI/CD[fn::continuous integration/continuous delivery]
   functionality via GitHub Actions[fn::https://github.com/features/actions].
   This enables to automatically run static analysis or automated tests whenever
   some predefined event occurs, such as creating a new pull request.  Another
   possibility is to add a linter directly to the build process and then trigger
   the build using the GitHub Actions.  Trautsch et al.[[cite:pmd]] analyzed several
   open-source projects in regards to the usage of static analysis tools.  They
   found out that incorporating a static analysis tool in a build process
   reduces the defect density.
* Pull request acceptance
  Pull request acceptance is a problem that has been studied multiple
  times. Several surveys were performed in order to understand why pull requests
  are being rejected.

  Gousios et al.[[cite:integrator]] surveyed hundreds of integrators to find out
  their reasons behind the PR rejection. Code quality was stated as the main
  reason by most of the integrators; code style was in the second place.
  Factors that integrators examine the most when evaluating the code quality are
  style conformance and test coverage.

  Kononenko et al.[[cite:shopify]] performed a study of an open-source project
  called /Shopify/; they manually analyzed PR's and also surveyed /Shopify/
  developers. They found out that developers associate the quality of PR with
  the quality of its description and with the revertability and complexity of
  the PR.

  The reasons why contributors abandon their PRs were also
  studied[[cite:abandonment]]. Reason number one was the "Lack of answers from
  integrators."; moreover, the "Lack of time" and the "Pull request is
  obsolete" was also often stated as the main reason.

  Even though the different open-source communities can approach the pull request acceptance in
  a different manner, three main governance styles can be
  identified --- protective, equitable, and lenient. The protective governance style
  values trust in the contributor-maintainer relationship. The equitable
  governance style tries to be unbiased towards the contributors, and the
  lenient style prioritizes the growth and openness of the community[[cite:foss]].
  Each style focuses on different aspects of PR. Tsay et al.[[cite:social]]
  identified the following levels of social and technical factors that influence
  the acceptance of the PR --- /repository level/, /submitter level/, and the
  /pull request level/.
** Repository level
   The /repository level/ is interested in the aspects of the repository itself,
   such as the repository age, number of collaborators, or number of stars on
   the GitHub.

   For instance, the programming language used in the project also influences
   the acceptance of the PRs. Pull requests containing Java, JavaScript, or C++
   code have a smaller chance of being accepted than PRs containing the code
   written in Go or Scala[[cite:factors]].

   Furthermore, older projects and projects with a large team have a
   significantly lower acceptance rate[[cite:social]].

   The popularity of the project also influences the acceptance rate ---
   projects with more stars have more rejected PRs[[cite:social]].
** Submitter level
   The /submitter level/ is concerned about the submitter's status in the
   general community and his status in the project itself. There are several
   parameters that can be considered when evaluating the submitter's status.

   PRs of submitters with higher social connection to the project have a higher
   probability of being accepted[[cite:social]].

   Submitter status in the general community plays an important role in PR
   acceptance. If the submitter is also a project collaborator, the likelihood
   that the PR will be accepted increases by 63.3%[[cite:social]].

   Moreover, users that contributed to a larger number of projects have a higher
   chance that their PR will be accepted[[cite:npm2]]. The acceptance of the new
   pull request also correlates with the acceptance of other older pull requests
   created by the same submitter[[cite:npm]][[cite:replication]]. Furthermore,
   the first pull requests of users are more likely to be rejected[[cite:developers]].

   The gender of the submitter is another factor that plays a role in PR
   acceptance. A study showed that woman's PR are accepted more often, but only
   when they are not identifiable as a woman[[cite:gender]].

   Personality traits also influence PR acceptance. The /IBM Watson Personality
   Insights/ were used to obtain the personality traits of the PR submitters by
   analyzing the user's comments. These traits were then used to study PR
   acceptance. It has been shown that conscientiousness, neuroticism, and
   extroversion are traits that have positive effects on PR acceptance. The
   chance that PR will be accepted is also higher when the submitter and closer
   have different personalities[[cite:personality]].
** Pull request level
   The /pull request level/ is interested in the data about
   PR itself.  For instance, on the /PR level/, one can study if there is
   a correlation between PR acceptance and the number of GitHub comments in
   the PR.

   One of the factors that negatively influence the acceptance rate is the
   number of commits in the pull request. The high number of
   commits decreases the probability of acceptance. On the other hand, PRs with
   only one commit are exceptions --- they have a smaller chance of being accepted
   than pull requests which contain two commits[[cite:npm2]].

   Another observation is that more discussed PRs have a smaller chance of being
   accepted[[cite:social]].  Another study did not find a large difference between
   accepted and rejected PRs based on the number of comments but found that
   discussions in rejected PRs have a longer duration[[cite:discussion]].
   Moreover, the increasing number of changed lines decreases the
   likelihood of PR acceptance[[cite:social]].

   The code quality is an essential factor on the /pull request level/, and it
   is this study's main interest.  The code quality as the acceptance factor is
   examined in the following subchapter.
*** Code quality
    One of the tools that ensure that the code has high quality is testing.
    Proper testing is a crucial part of every project.  Testing plays a
    significant role in discovering bugs and therefore leads to higher code
    quality.  One study found that PRs, including more tests, have a higher
    chance of being accepted[[cite:social]]. However, another study yields no
    relation between acceptance and test inclusion[[cite:exploratory]].

    Another factor that is closely tied to code quality is the code style.
    Proper and consistent code style increases the maintainability of the
    software.  The code style inconsistency has a small (but not negligible)
    negative effect on acceptance. PRs with larger code style inconsistency
    (with the codebase) have a smaller chance of being accepted.  Code style
    inconsistency also negatively influences the time required to close a
    PR[[cite:style]].

    Although many integrators view code quality as the most important factor
    regarding PR acceptance[[cite:integrator]], to the best of my knowledge, only
    one study[[cite:quality]] was performed to discover whether there is a
    connection between the PR's acceptance and the quality flaws found in the
    code (taking into account more complicated aspects than code style or test
    inclusion).

    Lenarduzzi et al. analyzed 28 open-source projects. The results show that
    there is no significant connection between code quality and PR acceptance.
    The key difference (from my thesis) is that they analyzed only projects written
    in Java. Furthermore, my thesis investigates the connection between the time to close a
    PR and the PR quality (unlike them). Further comparison is at the end of the thesis.
* Data mining
  #+BEGIN_EXPORT latex
  \begin{figure}[htb]\centering
  \begin{tikzpicture}
  \node (n1) [align=center] {Project name};

  \node (n2) [box, above=2cm of n1, align=center] {\texttt{gh\_db.py}\\(\texttt{gh\_rest.py})};
  \node (n3) [cloud, draw, above=of n2, align=center, inner sep=-3mm] {GHTorrent database\\(GitHub REST API)};
  \node (c1) [container, fit=(n2)(n3)] {};

  \node (n4) [right=2cm of n2, align=center, margin] {Pull requests\\information};

  \node (n5) [box, right=of n4] {\texttt{git-contrast}};
  \node (n6) [cloud, draw, above=of n5] {Linters};
  \node (c2) [container, fit=(n5)(n6)] {};

  \node (c3) [container, thick, fit=(c1)(c2)] {};

  \node (n7) [below=2.5cm of n5, align=center] {JSON};

  \node [below left, inner sep=3mm] at (current bounding box.north east) {\texttt{pr\_quality.py}};

  \draw[->] (n1) to (n2);
  \draw[<->] (n2) to (n3);
  \draw[->] (n2) to (n4);
  \draw[->] (n4) to (n5);
  \draw[<->] (n5) to (n6);
  \draw[->] (n2) edge node[sloped, below, align=center, font=\fontsize{8pt}{8pt}\selectfont] {Project\\information} (n7);
  \draw[->] (n5) edge node[right, yshift=-4mm, align=center, font=\fontsize{8pt}{8pt}\selectfont] {Pull requests\\code quality} (n7);
  \end{tikzpicture}
  \caption{The \texttt{pr\_quality.py} workflow}\label{fig:mining_workflow}
  \end{figure}
  #+END_EXPORT
  Information about the pull requests is retrieved using the =pr_quality.py=
  script. This script takes the names of the projects that will be analyzed as the
  input, and it outputs the JSON files containing information about the projects
  and their code quality (Figure\nbsp{}\ref{fig:mining_workflow}). The script
  needs to retrieve the metadata for each project and its pull requests. There
  are two possible sources that can be used: GitHub REST API and the GHTorrent
  database. Which source will be used can be specified by passing an argument to
  the tool. Metadata are then used to determine which objects need to be
  fetched from the GitHub to perform the code quality analysis. The analysis of
  the pull request itself is performed by an external tool called =git-contrast=.

  The =gh_db.py= is a script responsible for querying the GHTorrent database in order to
  obtain data about the projects. The GHTorrent database[[cite:ghtorrent]] is an offline mirror of
  data offered through the GitHub REST API. =gh_db.py= returns a JSON file
  with the information about the project, such as the number of stars, number of
  contributors, or information about pull requests and their commits.

  An alternative script that can be used by =pr_qality.py= is =gh_rest.py=.
  This script uses the GitHub REST API directly. The advantage of this
  script is that it can retrieve the newest data from GitHub. Unfortunately,
  the REST API is limited by the number of requests per hour. Because of that,
  the =gh_rest.py= is programmed to retrieve only a subset of data that are
  obtained by =gh_db.py= (data not crucial for the analysis are
  omitted).

  However, GitHub lacks information about the code quality of
  the pull requests. This is where the =git-contrast= comes into play.
  =git-contrast= is the command-line application that analyzes the code quality
  of the given pull request using the external linters. This application is
  further discussed in the following sections.
** GitHub metadata
   As stated before, the scripts =gh_db.py= and =gh_rest.py= are used
   to retrieve data from GitHub. GitHub can be leveraged to obtain
   many interesting metadata, which can possibly influence the acceptance of pull
   requests. All the metadata that are obtained using the scripts are listed
   in Table [[table:ghdata]].
   #+CAPTION: Data retrieved from GitHub
   #+LABEL: table:ghdata
   #+ATTR_LaTeX: :align |llcc| :placement [h]
   |--------------------+---------------------------+------------+--------------|
   | Level              | Metadata                  | =gh_db.py= | =gh_rest.py= |
   |--------------------+---------------------------+------------+--------------|
   |--------------------+---------------------------+------------+--------------|
   | Repository level   | Project name              | \ding{51}  | \ding{51}    |
   |                    | Programming language      | \ding{51}  | \ding{51}    |
   |                    | Time of creation          | \ding{51}  | \ding{51}    |
   |                    | Number of forks           | \ding{51}  | \ding{51}    |
   |                    | Number of commits         | \ding{51}  | \ding{55}    |
   |                    | Number of project members | \ding{51}  | \ding{55}    |
   |                    | Number of stars           | \ding{51}  | \ding{51}    |
   |--------------------+---------------------------+------------+--------------|
   | Submitter level    | Username                  | \ding{51}  | \ding{51}    |
   |                    | Number of followers       | \ding{51}  | \ding{55}    |
   |                    | Status in the project     | \ding{51}  | \ding{51}    |
   |--------------------+---------------------------+------------+--------------|
   | Pull request level | Pull request ID           | \ding{51}  | \ding{51}    |
   |                    | Is PR accepted?           | \ding{51}  | \ding{51}    |
   |                    | Time opened               | \ding{51}  | \ding{51}    |
   |                    | Head repository           | \ding{51}  | \ding{51}    |
   |                    | Head commit               | \ding{51}  | \ding{51}    |
   |                    | Base commit               | \ding{51}  | \ding{51}    |
   |                    | Number of commits         | \ding{51}  | \ding{55}    |
   |                    | Number of comments        | \ding{51}  | \ding{55}    |
   |--------------------+---------------------------+------------+--------------|

   Metadata like "Number of stars" or "Time opened" are required for the
   statistical analysis.  Others are not meant to be used as a part of the
   analysis itself but are kept here for better orientation, and some of them
   are needed for the =git-contrast= tool, such as "Head commit", "Base commit", etc.
** Evaluating code quality
   =git-contrast= is the command-line application that I implemented in order to
   be able to analyze the code quality of the given pull request. The =git-contrast=
   expects two commit hashes on the input and returns the information about the
   change in code quality between these commits on the output.
   The number of found code quality issues is
   then written to the standard output.

   To measure the change in the quality of the pull request, the
   =git-contrast= is run on the "head commit" and the "base commit" of the given
   pull request. The =git-contrast= supports several linters; which linter will be
   used is determined by the file extension of the tested file (Table [[table:linters]]).
   #+CAPTION: Linters supported by =git-contrast=
   #+LABEL: table:linters
   #+ATTR_LaTeX: :align |lcll|
   |--------------+---------+----------------------+-----------------------|
   | Linter       | Version | Programming language | File extensions       |
   |--------------+---------+----------------------+-----------------------|
   |--------------+---------+----------------------+-----------------------|
   | [[https://pylint.pycqa.org/][*Pylint*]]     |  2.12.2 | Python               | =.py=                 |
   | [[https://pmd.github.io/][*PMD*]]        |  6.42.0 | Java                 | =.java=               |
   | [[https://ktlint.github.io/][*ktlint*]]     |  0.43.2 | Kotlin               | =.kt= and =.kts=      |
   | [[https://github.com/ndmitchell/hlint][*HLint*]]      |   3.2.8 | Haskell              | =.hs=                 |
   | [[https://dwheeler.com/flawfinder/][*flawfinder*]] |  2.0.19 | C/C++                | =.c=, =.cpp= and =.h= |
   |--------------+---------+----------------------+-----------------------|

   The most problematic was to statically analyze the C/C++ source files because
   some linters also need the information on how the source code should be
   compiled. I tested the OCLint and Cppcheck linters but without success.
   The compilation flags cannot always be automatically determined from the makefiles.
   Because of that, I settled on using the flawfinder, which performs a simpler analysis and
   does not require compilation flags.

   The following linters are supported by =git-contrast=:
   - Pylint :: Python linter that is able to detect programming errors and helps
     enforce coding standards[fn::https://peps.python.org/pep-0008/].
     Issues are divided into the following categories: conventions, code smells,
     warnings (Python-specific problems), and errors.
   - PMD :: Linter that is able to discover common programming flaws. It is mainly
     concerned with Java and Apex programming languages. PMD is extensible but also
     provides many predefined rulesets: "Best Practices", "Code style", "Security"\dots
     All Java rule sets available in the basic installation were used to evaluate code quality.
   - ktlint :: Simple static analyzer focused on the code clarity and community
     conventions[fn::https://kotlinlang.org/docs/coding-conventions.html].
     This linter uses only a small set of carefully selected rules.
   - HLint :: Tool for suggesting possible improvements to Haskell code.
     Every hint has one of the following severity levels: error, warning, and suggestion.
   - flawfinder :: A simple program that examines C/C++ code and searches for potentially
     dangerous functions. This is done using the built-in database of functions with
     well-known problems. Linter uses the following risk levels: note, warning, and error.
** Projects selection
   In total, 100 projects were selected written in five different
   programming languages (20 projects for each language).  The analyzed GitHub
   projects were selected based on the following criteria:
   - The primary programming language is Python, Java, Kotlin, Haskell, or C/C++.
   - The project is popular --- it is in the top 150 most favorite projects written in the given language.
     One of the reasons to analyze popular projects is the fact that popularity influences acceptance[[cite:social]].
     Popular projects also usually contain a high number of pull requests.
     Two different lists of popular projects were used: projects sorted by the
     number of stars using the GHTorrent database (data from \nth{1} June 2019) and the list from
     GitHub[fn::https://github.com/EvanLi/Github-Ranking] (data from \nth{1} January 2022).
   - The project contains at least 200 pull requests that are suitable for analysis.
     This means that PR needs to contain at least one file written in the
     primary language and the data about PR needs to be publicly available.
   - The project is using GitHub to merge pull requests (for most of the pull requests).
   - The project is a library, program, or collection of programs. Repositories whose primary purpose is
     to store configuration files, documentation, books, etc., were ignored.
** TODO Computational resources
* Data analysis
  In this chapter, I am explaining which statistical methods were chosen in
  order to answer the research questions.  Research questions 1 to 4 were
  analyzed separately for each programming language; therefore, also the
  techniques that will be discussed were applied separately.  Only the last
  research question discuss multiple languages at the same time and compares
  results retrieved from the individual analysis of each language.
*** Which code issues are typically introduced by the pull requests?
    At first, in order to answer the *RQ_1*, I summarized the retrieved data for each project
    --- I counted how many suitable pull requests were analyzed and
    how many of them were accepted/rejected. Then I created a scatter plot between the number of
    stars and the percentage of accepted PRs.

    I also summarized all pull requests regardless of their project. I computed the average number
    of introduced issues, fixed issues, etc. Then I created a heat map that shows how many PRs
    introduced/fixed some specific number of issues.

    Then for each issue individually, I computed how many accepted/rejected pull
    requests introduced/fixed this issue, how many times this issue occurred in
    some pull request, etc. I created multiple lists of issues sorted by various parameters.
    I sorted issues by the number of rejected/accepted PRs that fixed/introduced them.
    I also listed issues and the percentage of PRs that changed their quality. I examined the
    issues that were fixed in a larger number of PRs than introduced. Then I created a scatter plot
    that shows which issue category is the most common.

    These steps were applied individually for each programming language to determine
    how does the average PR look line in terms of code quality.
*** Are there some particular quality flaws that affect the acceptance of the pull request?
    In order to discover issues that affect the acceptance of pull requests
    most, the classification models were created.  The aim of these models is to
    classify pull requests into two groups (accepted PRs and rejected PRs) by
    using the information about the quality change in the given pull
    request. Multiple classification algorithms were
    used[fn::https://scikit-learn.org/stable/modules/classes.html]:
    - LogisticRegression :: Despite its name, logistic regression is a linear model used for classification. It uses
      a so-called /logistic function/ that turns the inputs (code quality issues)
      into the probability of the dependent variable (PR acceptance) being 1 (PR is
      accepted).
    - DecisionTrees :: This algorithm constructs the tree where leaves represent the different classes (PR accepted/rejected),
      and inner nodes represent the so-called /split criterion/ --- the condition
      (or predicate) on single/multiple attributes (code quality issues).
      The /split criterion/ defines to which subtree given input (pull
      request) belongs.
    - Bagging :: The Bagging algorithm is trying to predict the data class (PR being rejected/accepted)
      using multiple different classifiers. It uses bootstrapping[fn::random sampling with replacement]
      to construct the different data sets for each
      classifier. The outputs from these classifiers are then aggregated to form
      the final prediction.
    - RandomForest :: This classifier leverages the bagging method in order to create the forest of
      uncorrelated decision trees (to avoid bias and overfitting). Unlike the decision trees,
      the RandomForest uses only a subset of features (code quality issues) to generate the decision tree
      (this ensures the low correlation between the trees).
    - ExtraTrees :: ExtraTrees is a classifier similar to RandomForest.
      The main difference is that the ExtraTrees algorithm generates /split
      criterions/ using randomization.  Another key difference is that
      ExtraTrees uses whole original sample for each tree (instead of
      bootstrapping).
    - AdaBoost :: The AdaBoost is another algorithm that leverages multiple weak classifiers (usually DecisionTrees with only one
      /split criterion/) to predict the final result. It begins by fitting a
      classifier on the original dataset. Each subsequent classifier is
      improved using the results from the previous one (incorrectly classified
      pull requests have a higher chance of being selected in the next
      classifier).
    - GradientBoost :: The GradientBoost algorithm is similar to the AdaBoost. It is also
      using multiple weak classifiers, and they are trained one by one. However,
      instead of improving the
      subsequent classifier by changing the training dataset distribution, the GradientBoost algorithm
      trains the classifiers using the residual errors of predecessors. Furthermore, the GradientBoost
      works with larger trees than AdaBoost.
    - XGBoost :: XGBoost is a popular variant of gradient boosting. It is designed to be fast
      and efficient.
    Each of those algorithms was run on three different datasets --- a dataset
    with quality change, a dataset containing only introduced issues, and a dataset
    with only fixed issues. In the first dataset, the quality change for some issues was
    represented by the integer, and this integer was negative if the issue was fixed in the PR
    and positive if the issue was introduced. The other datasets were created by filtering
    positive/negative values from the first dataset. Running the classification algorithms on
    the dataset with only fixed issues can help to understand if the improvement in code quality
    can also influence the acceptance.

    In order to recognize issues that have some effect on the PR acceptance,
    the /drop-column importance/ mechanism[fn::https://explained.ai/rf-importance/] was used.
    This mechanism is resource-intense (requires a lot of computational power) but is usually more reliable
    than the classic importance mechanisms.

    The dataset was split into five parts to better evaluate the model accuracy
    (5-fold cross-validation).  Each model was then trained five times ---
    a distinct dataset was used for training and for validation.  Several metrics
    (precision, recall, AUROC, F-measure\dots) were used to evaluate the
    reliability of each model. Afterward, the average metrics over all folds
    were computed.

    The same technique was used by Lenarduzzi et al.[[cite:quality]]. The script
    they provided (slightly modified) was used to run the classification
    algorithms.
*** Is there a relationship between the source code quality and the pull request acceptance?
    At first, the PCA (principal component analysis) scatter plot was created to
    visualize the difference between accepted and rejected pull requests.

    The impact of the presence of some code issue in the PR on the PR acceptance was
    determined using the $\chi^2$ test. In order to perform this test, the dataset
    was transformed into a /contingency table/.  This table ($2 \times 2$) contained
    the number of accepted/rejected PRs with/without a code quality issue.
    After that, the $\chi^2$ test of independence was performed on the
    /contingency table/.  The /significance level/ was set to $\alpha =
    0.05$. However, relying only on statistical significance can be misleading
    because it is affected by sample size. To understand the practical
    significance of the test (/effect size/), the Cramer's V denoted as $\phi_c$
    was also computed. The Cramer's V ranges between 0 (no
    association) and 1 (complete association).

    Pull request that adds or removes some files greatly influences
    code quality. If the number of removed/added files has a large impact on PR
    acceptance (regardless of code quality), then it can be a large threat to
    the validity of the independence test.  The pull request acceptance can also be
    influenced by the quality of files which were not linted (were written in
    non-primary language).  To eliminate the risk that the test was influenced,
    the same test was performed on pull requests that only modified some source
    files, and these files were written in the primary language.

    Moreover, the $\chi^2$ test was performed independently for each issue
    category to understand if there are some issue categories that have a
    stronger influence on the quality.

    The test was also computed for each project separately. Unluckily, there are
    some projects that contain an insufficient number of pull requests.  According
    to Cochran[[cite:cochran]], all expected counts should be ten or greater.
    Therefore, the tests were performed only on some projects (that have a sufficient
    number of expected counts).

    It is important to note that p-values were not adjusted in any way.

    The metrics obtained from classification algorithms were also used to
    determine if the code quality has some impact on PR acceptance.
*** Does code quality influence the time required to close a pull request?
    In order to find the possible link between the code quality and the time it
    takes to close a PR, regression algorithms were used. At first, the
    dataset was split into two parts --- training and test set.  After that, the
    regression model was trained on the training set. Then, the importance of
    individual quality issues was determined using the /permutation importance/
    mechanism. Afterward, the model was used to predict the
    time based on the data from the test set. Metrics such as /mean absolute error/
    (MAE), /mean squared error/ (MSE), and /coefficient of determination/ ($R^2$)
    were computed using the predicted and expected values and used to evaluate the
    models.

    Following regressors were used[fn::https://scikit-learn.org/stable/modules/linear_model.html]:
    - LinearRegression :: Linear regression is a commonly used type of predictive model.
      It is used for modeling the linear relationship between explanatory variables (code quality issues)
      and a scalar response (time to close a PR). The model that minimizes the residual sum of squares
      is selected.
    - ElasticNet :: ElasticNet is an extension of linear regression. It is adding $L_1$ (lasso regression)
      and $L_2$ (ridge regression) penalties in order to make the linear model more robust.
      The problem with the classic linear regression is that the estimated coefficients can be
      too high due to overfitting. Because of that, the model parameters are added to the
      /loss function/[fn::a function that is minimized during the regression] as a penalty.
    - Some of the already discussed methods used for classification were also used for regression.
      Following methods were used for both classification and regression:
      *DecisionTree*, *RandomForest*, *AdaBoost*, *Bagging*, and *GradientBoost*.
*** Is code quality impact higher in projects that are using some particular programming language?
    The *RQ3* discusses the impact of code quality on individual
    programming languages. The findings from the *RQ3* for each
    language are compared in the *RQ5*. This comparison is a complicated
    task because each language has different characteristics, and
    a different linter was used to measure its code quality.

    The results from $\chi^2$ tests were compared to identify
    the possible difference between the languages (in terms of code
    quality). The metrics retrieved from classification models were
    also compared. Finally, the code quality effect on the time to close a PR
    was compared between the languages (using the metrics from regressors).
* Evaluation
  The following chapter is dedicated to the findings from my research.  The first
  five subchapters focus on individual programming languages --- here I am
  giving the answers to the first four research questions.  The last research
  question (*RQ5*) is answered afterward. At the end of this chapter, I am
  discussing possible threats to validity that could eventually influence the
  outcomes of my study.
** Python
   In order to analyze the influence of code quality on the pull request
   acceptance, 20 projects from the Python ecosystem were selected.
   In total, 9452 pull requests were analyzed, and 73 % of these PRs were accepted.
   Pull requests were more accepted in less popular projects, as can be seen in
   the following scatterplot:
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth + 2cm}{!}{%
     \input{results/python/stars_and_acceptance}
   }
   \caption{Stars and pull request acceptance}
   \end{figure}
   #+END_EXPORT

   On average, one pull request introduced 5.36 issues and fixed 2.44 issues;
   an accepted pull request introduced 4.62 and fixed 1.99 issues, and rejected
   pull request introduced 7.86 issues and fixed 4.43 on average.
   5 % trimmed mean was used to compute these values.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth + 2cm}{!}{%
     \inputTikz{results/python/}{pr_quality_heat_map}
   }
   \caption{Pull requests and quality}
   \end{figure}
   #+END_EXPORT

   In the analyzed pull requests, Pylint detected 222 different issues.

   The conventions dominated the list of issues that were fixed/introduced in
   the largest number of pull requests.  The convention that was
   fixed/introduced in the largest number of pull requests is
   =missing-function-docstring= (in 37 % of PRs); conventions
   =invalid-name=, =line-too-long= and =consider-using-f-string= were
   fixed/introduced in over 20 % of pull requests. There were 15 issues
   that were fixed/introduced in more than 10 % of PRs, and 72 issues were in
   over 1 % of PRs (out of the 222 issues which were found in the pull
   requests).  There were nine issues that were present in the analyzed pull
   requests but did not influence their quality (the number of these issues was not
   changed by any pull request). 13 issues were introduced/fixed in only one
   pull request, and 10 of them are issues classified as errors. The most common
   error is =import-error= (24 % of PRs); however, I suspect that there will be
   many false positives that arise due to linting in the isolated
   environment. Sixty issues were fixed in more PRs than they were introduced.
   They are 24 more PRs that fixed the warning =super-init-not-called= than the
   PRs that introduced it.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth + 2cm}{!}{%
     \input{results/python/issues_types_and_prs}
   }
   \caption{Pylint issues and \% of PRs which fixed/introduced them}
   \end{figure}
   #+END_EXPORT

   The most important Pylint issue in regards to the PR acceptance is the
   =syntax-error=.  XGBoost classifier gives this error the 1.2 %
   importance. However, other classifiers consider this error less important.
   On average importance of the =syntax-error= is only 0.3 %.  The syntax error
   was introduced in 17 projects. On average, rejected pull request introduced
   =0.027= syntax errors, and the average accepted pull request even fixed =0.001=
   syntax errors.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \input{results/python/issue_importance}
   \caption{Ten most important Pylint issues}
   \end{figure}
   #+END_EXPORT

   When only introduced issues were considered, the list
   of the most important issues looked differently. On the other hand, there
   are some issues that appeared in the top 10 in both lists: =syntax-error=,
   =unused-variable= and =unused-import=. The =syntax-error= is considered
   the most important issue by both methods.

   When only the information about fixed issues is used, the most important issue
   is =f-string-without-interpolation= (in terms of acceptance). However, no classifier
   gives this issue importance over one percent.

   In order to visualize the difference in quality between accepted and rejected PRs, I created PCA scatter plot:
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \input{results/python/acceptance_pca}
   \caption{PCA scatter plot}
   \end{figure}
   #+END_EXPORT
   In the PCA scatter plot, there is no visible difference between rejected and accepted pull requests.

   To understand if the presence of some issue in the PR influences its acceptance, I created contingency matrices
   and performed a $\chi^2$ test of independence:
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth}{!}{%
     \input{results/python/acceptance_ct}
   }
   \caption{Relation between presence of issue and PR acceptance}\label{fig:python_ct}
   \end{figure}
   #+END_EXPORT

   As can be seen in Figure\nbsp{}\ref{fig:python_ct}, the observed number of
   rejected pull requests which contained some defected is higher than
   expected. For $\chi^2$ test, $p < \num{2.2e-16}$ and therefore, the
   hypothesis that presence of some issue and PR acceptance are independent is
   rejected on significance level $\alpha = 0.05$. However, the Cramer's $\phi_c
   \approx 0.092$; therefore, the association between issue presence and acceptance is weak.
   This conclusion also supports the fact that AUROC for trained classification models is only slightly over 0.5.
   The average AUC for all models is $0.534$.

   When considering only PRs that solely modified some source files, $p < \num{5.548e-10}$
   and therefore also here the presence of some code quality issue in the PR
   influences the PR acceptance.
   Similar to the previous test, the $\phi_c \approx 0.087$; therefore, the
   association between the presence of the same issue and PR acceptance is weak.

   Almost identical results were obtained when the $\chi^2$ test was performed separately for each issue category.

   When the projects were considered individually, only for nine of them the $p < \alpha$. In these projects,
   the poor code quality had a negative impact on PR acceptance.
   In the rest of the projects, the presence of some code quality issue does not seem to have an effect on
   the PR acceptance.

   The quality of the code does not seem to have an effect on the time it takes to close a pull request.
   All of the trained regression models have a negative $R^2$ score (when evaluated on the test set).
   This means that trained models are worse at predicting the time than a constant (mean value).
   Similar results were obtained when only introduced issues were considered and also when only
   fixed issues were considered.
** Java
   The next programming language that was analyzed is Java. In total, the 8887
   pull requests were linted, and 73 % of these pull requests were accepted.
   On average, the one pull request introduced 20 new PMD issues but,
   at the same time, also fixed 18 other issues.

   Like int the Python projects, the pull request from the less popular project
   were more likely to be accepted than pull requests from more popular projects.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth + 2cm}{!}{%
     \input{results/java/stars_and_acceptance}
   }
   \caption{Stars and pull request acceptance}
   \end{figure}
   #+END_EXPORT

   Only 1366 pull requests (from the total of 8887 pull requests) did not change
   the quality of the source code (did not fix nor introduce some PMD issues).
   The PMD linter was able to detect 253 different issues in the given pull requests.
   Most of the introduced issues were issues related to the code style. In total,
   all of the pull requests introduced over a million code-style issues.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth + 2cm}{!}{%
     \inputTikz{results/java/}{pr_quality_heat_map}
   }
   \caption{Pull requests and quality}
   \end{figure}
   #+END_EXPORT

   The issue that was introduced in the largest number of pull requests is
   =CommentRequired= (documentation issue).  Another frequent issues are
   =LocalVariableCouldBeFinal=, =MethodArgumentCouldBeFinal= (code style issues)
   and =LawOfDemeter= (issue in code design). These issues are the only issues
   that were introduced in more than 3000 pull requests. Similarly, the list of issues
   that were fixed in the largest number of the pull request is dominated by the
   very same issues.

   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth + 2cm}{!}{%
     \input{results/java/issues_types_and_prs}
   }
   \caption{PMD issues and \% of PRs which fixed/introduced them}\label{fig:pmd_issues_prs}
   \end{figure}
   #+END_EXPORT
   As can be seen in Figure\nbsp{}\ref{fig:pmd_issues_prs}, the documentation issues
   tend to appear in a large number of pull requests (24 % on average). Moreover,
   the typical code style issue appeared in 11 % of pull requests. On the other
   end of the spectrum, an average issue indicating an error-prone construct is present in only two
   percent of pull requests.

   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \input{results/java/issue_importance}
   \caption{Ten most important PMD issues}
   \end{figure}
   #+END_EXPORT
   The most important PMD issue is =JUnitAssertionsShouldIncludeMessage=. The
   average importance of this issue is only 0.6 %. However, the AdaBoost
   classifier gives this issue 3.7 % importance.  The 0.89 issues of this type
   are introduced in an average accepted pull request. I suspect that the pull
   requests that are adding a larger number of tests to the codebase have a higher
   probability of being accepted. At the same time, these pull requests also have a higher probability
   of introducing the =JUnitAssertionsShouldIncludeMessage=. This can be the
   reason why this issue has the largest importance.  This also supports the
   study that shows that the acceptance likelihood is increased by 17.1 % when
   tests are included[[cite:social]]. However, another performed study indicates that
   the presence of test code does not influence PR acceptance[[cite:exploratory]].

   The PCA scatter plot for Java pull requests looks as follows:
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \input{results/java/acceptance_pca}
   \caption{PCA scatter plot}
   \end{figure}
   #+END_EXPORT
   In the PCA scatter plot, there is no visible difference between rejected/accepted pull requests.

   To understand the relationship between acceptance and the introduction of a quality issue,
   the $\chi^2$ test was performed.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth}{!}{%
     \input{results/java/acceptance_ct}
   }
   \caption{Relation between presence of issue and PR acceptance}
   \end{figure}
   #+END_EXPORT
   The $p = \num{9.132e-14} < \alpha$ and $\phi_c = 0.079$; therefore, there is
   a weak relation between acceptance and issue presence. Similar results were
   obtained when only PRs that solely modified the source code of the main language were
   considered and also when the test was performed individually for each issue category.

   17 out of the 20 Java projects contained a sufficient number of pull requests to
   perform the $\chi^2$ tests. In nine of them, the code quality and acceptance are
   not independent. Unexpectedly, in one of the projects (=alibaba/fastjson=) the
   presence of an issue has a small positive effect on the acceptance.

   The PMD issues seem to have some effect on the time it takes to close a pull
   request when considering only $R^2$ computed for each model. However, the
   $R^2$ value is usually not a good metric for evaluate non-linear models;
   it can reveal some information about the model, but it does not give us
   information on how accurate the model is. There are three models that have $R^2
   > 0.4$: Bagging, GradientBoost and RandomForest.  The linear regression has
   $R^2 = 0.1257$; therefore for this model, 13 % of the variance in time to close a
   PR can be explained by quality issues. However, all of the models have high mean
   absolute error (MAE). The average MAE value for all of the models is $3934338
   \approx 46\text{ days}$ and 87 % of all analyzed Java pull requests were
   closed within one month. Therefore these models are basically useless in
   practice. The other models (when considering only rejected/fixed issues) yielded
   similar results. To conclude, the found quality issues do not seem to have an
   effect on the time to close a pull request.
** Kotlin
   The 20 projects were also selected from the Kotlin ecosystem.
   The average analyzed pull request was from a project that has ten thousand
   stars and introduced nine issues and fixed only four. The 7514 pull requests
   were analyzed (using the /ktlint/ linter), and 80 % of them were accepted.
   The trend that maintainers of popular projects reject more pull requests can
   also be observed in the Kotlin community.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth + 2cm}{!}{%
     \input{results/kotlin/stars_and_acceptance}
   }
   \caption{Stars and pull request acceptance}
   \end{figure}
   #+END_EXPORT

   Only 20 different issues were detected by the /ktlint/ in the analyzed projects;
   however, this is expected since the /ktlint/ is focused only on a small set of quality issues.

   The =indent= is the issue that was introduced in the largest number of pull requests (2598). It is the
   only issue that was introduced in more than a thousand pull requests. It is also the issue
   that was fixed in the largest number of pull requests. The official Kotlin convention is
   to use the four spaces for indentation[fn::https://kotlinlang.org/docs/coding-conventions.html],
   and the =indent= issue signifies that this convention was violated. This issue influenced
   the code quality of more than half of the pull requests. However, this can be caused by projects
   whose standards do not follow the official recommendations.

   Other often violated /ktlint/ rules are =no-wildcard-imports=, =final-newline=, and =import-ordering=.
   On the other end of the spectrum, the rule =no-line-break-after-else= was violated only once.

   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth + 2cm}{!}{%
     \inputTikz{results/kotlin/}{pr_quality_heat_map}
   }
   \caption{Pull requests and quality}
   \end{figure}
   #+END_EXPORT

   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \input{results/kotlin/issue_importance}
   \caption{Ten most important ktlint issues}
   \end{figure}
   #+END_EXPORT
   The issue with the highest importance average is =dot-spacing=.
   The Bagging classifier gives 1.7 % importance to this issue. The importance obtained from other
   classifiers is smaller --- the average importance is 0.8 %.
   However, this issue was introduced only in 18 PRs (13 times in the rejected pull request).
   Furthermore, seven accepted and seven rejected pull requests fixed this issue.
   Therefore the impact of this issue is disputable.

   It is worth mentioning that fourth most important issue does not have a name
   (given by /ktlint/).  This issue usually indicates an invalid Kotlin file.  This
   issue has high importance (relative to the other issues) also when the only
   fixed and also when only introduced issues were taken into account during the
   classification. This issue was introduced by 90 rejected PRs and by 51
   accepted PRs.

   When using only introduced issues, the most important issue is =indent=.
   This issue is also most important when only the fixed issues are considered.
   As being said before, in projects that are using non-standard indentation,
   this issue is a false positive.

   The PCA scatter plot was also created for the Kotlin programming language.
   The first principal component explains almost all variance in the code quality of pull requests.
   However, the difference between rejected/accepted pull requests is not apparent from the PCA plot:
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \input{results/kotlin/acceptance_pca}
   \caption{PCA scatter plot}
   \end{figure}
   #+END_EXPORT

   To understand the link between acceptance and the introduction of some
   quality issue, I performed the $\chi^2$ test on Kotlin dataset. The $p < \num{2.2e-16}$ and
   $\phi_c \approx 0.095$; therefore, the presence of some issue has a small
   negative effect on acceptance (similarly to the Java and Python).
   Furthermore, three classifiers (/Bagging/, /GradientBoost/, and /RandomForest/)
   have AUC for ROC curve above 60, and the average AUC is $57.58$.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth}{!}{%
     \input{results/kotlin/acceptance_ct}
   }
   \caption{Relation between presence of issue and PR acceptance}
   \end{figure}
   #+END_EXPORT
   However, taking into account solely the PRs that only modified some source
   code, the $p = 0.627$, thus the acceptance and issue presence are independent
   (in this context).

   Only 12 of the projects have a sufficient number of pull requests to evaluate
   the $\chi^2$ test. There are four projects where the presence of some issue
   has a small impact on the PR acceptance (the average Cramer's V is $\phi_c = 0.18$).

   To analyze the relation between the code quality and time that is required to
   close a PR, I applied several regression techniques also to the Kotlin
   dataset.  For linear regression, $R^2 = 0.164$, therefore the trained model is
   able to explain 16 % of the variance in the time to close a PR. The $MAE =
   2375121 \approx 27\text{ days}$; therefore, the model does not perform so well
   on the dataset, taking into consideration that 89 % of pull requests were
   closed within one month. The mean absolute error for other models was similar
   to the $MAE$ obtained for linear regression.
** Haskell
   Haskell is the only purely functional programming language that was analyzed.
   The 18 out of 20 selected Haskell projects have under the 5000 stars. There
   are only two exceptions: PureScript with 7632 stars and Pandoc, which has over
   15000 stars. The Pandoc has the also smallest percentage of accepted pull
   requests.  However, excluding the Pandoc, there is no visible connection
   between the number of stars and acceptance in the selected projects. When the
   outliers are filtered, the trend tends to be the opposite of previous languages:
   more accepted are pull requests of projects with more stars.  However, only
   20 projects are not sufficient to make such conclusions about the whole
   population of Haskell projects.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth + 2cm}{!}{%
     \input{results/haskell/stars_and_acceptance}
   }
   \caption{Stars and pull request acceptance}
   \end{figure}
   #+END_EXPORT

   The 6949 pull requests were analyzed. Interestingly, in over 60
   % of pull requests, no change in the code quality was detected. Moreover, the
   /HLint/ is able to recognize a large number of different issues (321 issue
   types were detected in selected pull requests). On the other hand, some issues
   were counted twice because they appeared as a suggestion but also as a warning
   (in the different contexts).
   These facts can indicate that a large
   number of submitted pull requests follow high-quality standards.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth + 2cm}{!}{%
     \inputTikz{results/haskell/}{pr_quality_heat_map}
   }
   \caption{Pull requests and quality}
   \end{figure}
   #+END_EXPORT

   Seventy-eight percent of pull requests were accepted, and the average pull request introduced
   only 0.6 issues and fixed 0.3 issues. The most common types of issues were suggestions
   and warnings. The error that was introduced in the largest number of pull requests is
   =Use-newTVarIO=, and this error was introduced only in 8 pull requests. The most common
   suggestions were =Redundant-bracket= (introduced in 499 PRs) and =Redundant-$= (444 PRs).
   The warning =Unused-LANGUAGE-pragma= was introduced in 323 pull requests and =Eta-reduce=
   warning in 214 of them. There were only ten issues that were introduced in 100 and more
   pull requests; and another 105 issue types were detected in the analyzed code, but no PR introduced
   any of those issues.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth + 2cm}{!}{%
     \input{results/haskell/issues_types_and_prs}
   }
   \caption{HLint issues and \% of PRs which fixed/introduced them}
   \end{figure}
   #+END_EXPORT

   The most important Haskell issue is the suggestion =Use-if=. However, no classifier gives this
   issue importance over one percent. Therefore the actual impact of this issue is disputable.
   This issue was introduced in 18 rejected PRs and fixed in 11. There are 19 accepted PRs that
   introduced =Use-if= and 27 accepted PRs that fixed it.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \input{results/haskell/issue_importance}
   \caption{Ten most important HLint issues}
   \end{figure}
   #+END_EXPORT
   When only introduced issues were taken into account, the most important issue is =Move-brackets-to-avoid-$= (suggestion).
   The AdaBoost classifier gives this issue 1 % importance, although the average importance is only 0.4 %.

   In the context of fixed issues, the most important is warning =Use-fewer-imports= with average importance again only about 0.4 %.

   The PCA scatter plot was also generated for the Haskell language.
   Similar to the results in already analyzed languages, there is no apparent
   difference between accepted and rejected pull requests.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \input{results/haskell/acceptance_pca}
   \caption{PCA scatter plot}
   \end{figure}
   #+END_EXPORT

   For the $\chi^2$ test, the $p = 0.001438 < \alpha = 0.05$ and Cramer's V is only $\phi_c = 0.038$;
   therefore, the presence of an issue in the PRs has only a small negative impact on the
   acceptance of the pull request. Similar results were obtained when only
   the pull requests that contain exclusively some modified code were considered.
   Furthermore, tests for the individual issue types also yielded similar results.
   Unfortunately, there is only a small number of pull requests that introduced some errors;
   therefore the $\chi^2$ test cannot be performed on this issue category.
   The average AUC computed for ROC curves is around 50 --- the classification algorithms
   were unable to distinguish between the accepted and rejected PRs using the code quality.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth}{!}{%
     \input{results/haskell/acceptance_ct}
   }
   \caption{Relation between presence of issue and PR acceptance}
   \end{figure}
   #+END_EXPORT
   The 13 projects contain a sufficient number of pull requests; the acceptance and
   the issue presence are not independent only in four of them (there, the issue presence
   have a small negative impact on the acceptance). For the =haskell/aeson= project,
   the Cramer's V is $0.282$ --- the association is "medium".

   The issues detected by /HLint/ do not seem to have an impact on the time it takes to close a pull request.
   All trained models have negative $R^2$. When only fixed issues were used for regression, there
   were three models with positive $R^2$: Bagging (0.0315), ElasticNet (0.0085), and RandomForest (0.0229).
   However, all of them have high mean absolute error: Bagging ($2193658 \approx 25\text{ days}$),
   ElasticNet (2255678), and RandomForest (2201347).
** C/C++
   The C and C++ programming languages are analyzed together because they share
   a lot of similarities.  This usually enables use of the same linter for both
   languages. Moreover, it is not uncommon that projects that are written in C++
   also contain some C code and vice versa.  The nine selected projects have more
   code written in C, while the rest of the 11 projects is more C++-oriented.

   In analyzed projects, there is no visible connection between the acceptance
   and the number of stars.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth + 2cm}{!}{%
     \input{results/c_cpp/stars_and_acceptance}
   }
   \caption{Stars and pull request acceptance}
   \end{figure}
   #+END_EXPORT

   I analyzed 8774 C/C++ pull requests. Seventy-seven percent of them have been accepted.
   The typical pull request introduces 0.25 issues and fixes 0.12 issues; the typical
   rejected PR introduces 0.79 issues, and the typical accepted PR only 0.15 issues.
   The 79 % of pull requests did not change the quality of the source code
   (in terms of the /flawfinder/ quality rules).
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth + 2cm}{!}{%
     \inputTikz{results/c_cpp/}{pr_quality_heat_map}
   }
   \caption{Pull requests and quality}
   \end{figure}
   #+END_EXPORT

   The most common type of issue is the note. The least common are errors. The
   /flawfinder/ was able to identify 137 different issues in the studied
   PRs. All of the top ten issues (in terms of number of PRs which introduced
   them) are notes. The most common note is =buffer-char= ("Statically-sized
   arrays can be improperly restricted leading to potential overflows or other
   issues\dots{}"). The most common error is =buffer-strcat= ("Does not check
   for buffer overflows when concatenating to destination\dots"), and it is the
   11 most introduced issue (introduced in 69 pull requests). There are 36 issues
   that were present in the analyzed code, but they were not introduced in any
   pull request; 21 of them are errors.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth + 2cm}{!}{%
     \input{results/c_cpp/issues_types_and_prs}
   }
   \caption{flawfinder issues and \% of PRs which fixed/introduced them}
   \end{figure}
   #+END_EXPORT

   Classification algorithms rank as the most important issue the
   =format-printf= ("If format strings can be influenced by an attacker, they
   can be exploited\dots"). However, this issue is only a /note/.  Therefore it
   does not have to indicate a defect (there will probably be a large number of
   false positives). AdaBoost and XGBoost algorithms give this issue importance
   of 1 %. The average importance is 0.7%.  This issue is also most important
   when only introduced issues are considered.  The second most important issue has
   average importance of only 0.26 %.

   The most important error is =buffer-StrCpyNA= ("Does not check for buffer
   overflows when copying to destination\dots") with average importance of only
   0.9 %. This error is the sixth most important issue.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \input{results/c_cpp/issue_importance}
   \caption{Ten most important flawfinder issues}
   \end{figure}
   #+END_EXPORT
   When considering only fixed issues, the =buffer-read= is the most important issue (note);
   however, the average importance is only 0.28 %.

   The PCA analysis does not reveal any significant difference between the accepted
   and rejected pull requests (in terms of code quality).
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \input{results/c_cpp/acceptance_pca}
   \caption{PCA scatter plot}
   \end{figure}
   #+END_EXPORT

   Based on the $\chi^2$ test, the presence of an issue in the PR has a small negative impact
   on the PR acceptance ($\phi_c = 0.117$).
   However, When considering only pull requests that solely modified some source files,
   Cramer's V $\phi_c = 0.024$ and $p = 0.1 > \alpha$ --- in this settings, the issue presence
   does not influence acceptance.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth}{!}{%
     \input{results/c_cpp/acceptance_ct}
   }
   \caption{Relation between presence of issue and PR acceptance}
   \end{figure}
   #+END_EXPORT
   Some small impact impact was discovered when the $\chi^2$ test was performed separately for
   each issue category (the $p < \num{2.2e-16}$ and $\phi_c \approx 0.1$ for each category).
   Furthermore, in 6 out of 11 projects which have enough data to perform and evaluate the $\chi^2$ test,
   the presence of some issue in the PR has a negative effect on the PR acceptance.
   In the =minetest/minetest= and =pybind/pybind11= projects, this effect is moderate;
   for other projects, the association is small.

   In the case of C/C++, the time to close a pull request seems not to be related
   to found issues.  All the models have negative $R^2$, except the ElasticNet
   regressor. For the ElasticNet, $MAE = 4681624$ (the mean absolute error is 54
   days) --- therefore, this model also cannot be used to predict the time to
   close a PR.  Models considering only rejected issues and also models
   considering only accepted issues have yielded similar results.
** Programming languages and code quality impact
   Comparing the code quality of projects written in different programming
   languages is a difficult task.  Each language has different programming
   constructs, syntax, and type system. For instance, Python, which is
   a dynamically-typed multi-paradigm programming language, has completely
   distinct characteristics from Haskell, which is a purely functional programming
   language with a strong, static type system.

   Moreover, every linter is different and has a unique set of rules.  The
   /ktlint/ is focused on code clarity and community conventions, whereas
   /flawfinder/ checks code for potentially dangerous functions. On the other
   hand, the /PMD/ is a more general-oriented linter that contains a large set of
   rules for the Java programming language. Lastly, the /HLint/ is oriented mainly
   on code simplification and spotting redundancies.

   On the other hand, there are some metrics that evaluate how effectively
   trained models predict the acceptance of PR or time to close a PR;
   and these metrics can be compared across different programming languages.
   On top of that, the results from the $\chi^2$ test can also be compared.
   However, the cation is in order because the code quality for each language is
   evaluated differently, as discussed before.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth}{!}{%
     \input{results/all_cramers_v}
   }
   \caption{Comparison of Cramer's V}\label{fig:all_v}
   \end{figure}
   #+END_EXPORT
   As can be seen in Figure\nbsp{}\ref{fig:all_v}, in all studied languages, the
   presence of some issue have a negative effect on the PR acceptance (in terms
   of $\chi^2$ tests); however, for all of the languages, this effect is small ($\phi_c
   \approx 0.1$).  The smallest effect was observed for Haskell programming
   language and the highest effect for C/C++.  On the other hand,
   taking into account solely the PRs that only modified some source code of the
   primary language, the $\chi^2$ test indicates that the presence of issue and PR
   acceptance are independent in the case of the C/C++ and Kotlin. This is a possible
   threat to validity.

   The effect of code quality on acceptance was also studied using
   classification algorithms.  One of the metrics that were used to measure the
   performance of the classification models is the "area under the ROC curve"
   (AUC). When using this metric to evaluate models, the Haskell is once again
   the language when the code quality is least important
   (Figure\nbsp{}\ref{fig:all_auc}). The average AUC for Haskell models is around
   0.5 --- the trained models are no better than random guessing.
   The models for the Kotlin are ranked with the highest AUC score and therefore
   are better in classification than models for other languages.
   Except for Haskell, the average AUC is over 0.5 but under 0.6 --- these AUC
   scores are usually considered poor[[cite:logreg]]. This indicates that code quality
   has only a small or no effect on the acceptance.

   As can be seen, similar results were obtained for all of the languages.
   In all of the languages, the code quality impact is small (based on the $\chi^2$ tests
   and also based on the results from classification algorithms). There is no language
   that significantly differs from others.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth}{!}{%
     \input{results/all_auc}
   }
   \caption{AUC for differenct languages (ROC)}
   \end{figure}\label{fig:all_auc}
   #+END_EXPORT

   As discussed in the previous chapter, there seems to be no connection between
   the code quality and the time it takes to close pull requests (based on the
   trained regression models).  The smallest MAE was scored by Kotlin models
   (around 26 days); on the other end of the spectrum are Python models with an
   average MAE equal to 78.9 days. The trained models are unusable, considering
   that most of the pull requests are closed within the first two weeks (83 % of Kotlin
   PRs and 76% of Python PRs).
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth}{!}{%
     \input{results/all_mae}
   }
   \caption{Mean absolute error for prediction of time to close a PR}
   \end{figure}
   #+END_EXPORT
** Threats to validity
   The validity of my research is endangered by several things.
   At first, the selection of the projects is one of the factors that influence
   the outcomes of the research. This study is focused primarily on popular
   projects. The rationale behind the project selection is explained in the own dedicated
   subchapter. It is possible that projects selected using different metrics can yield
   varying results.

   Another possible threat to validity is the selection of pull requests.
   It is usually not doable to examine all the pull requests of some project.
   For the projects with a huge number of PRs, the time and computational resources are
   the limiting factors. Moreover, to examine the rejected pull requests, the forked
   repository with the required commits needs to be available. This is not always the case.
   Sometimes the /force push/ can also remove the commits from the accepted pull requests.
   It is also important to note that linting of some pull requests resulted in an error in the
   linter, and therefore these PRs were skipped. Pull requests were also skipped if the
   linting time exceeded the limit (that was set to 1000 seconds) --- the PRs that
   modified a huge number of files were ignored. Furthermore, for some projects,
   the number of analyzed pull requests was limited to 500 to reduce the total
   time required for analysis.

   Another problem is that pull requests can be merged manually outside GitHub.
   These pull requests are not recognized as accepted[[cite:ghperils]]. The projects were selected so that
   GitHub is the primary way to merge PRs. However, there still can be some PRs
   merged using alternative methods.

   Furthermore, different methods can be used to measure the quality of pull
   requests. For each programming language, there exist several linters that
   are focused on a different set of issues, and they can also use different algorithms
   to detect the same issue. Another possible threat are false positives from linters.
   The false-positive can arise due to the fact that the files were linted in the
   isolated environment, and this can introduce some issues (=import-error=, etc.).
   Some issues are also hard to detect; for instance, the issue can be specific to
   some particular context, and the linter does not have to take this context into
   account. The greatest difficulty with the quality evaluation is the fact that
   everyone has a unique personal perspective on code quality --- code quality
   does not have a single definition.

   The pull requests sometimes contain also files that are not written in the
   primary programming language of the project. The pull request then can be
   rejected because of these files.

   Lastly, there are several factors that influence PR acceptance.
   Some of them were discussed in previous chapters (number of commits, submitter's status, etc.).
   The one factor that influences the acceptance is a number of lines that were changed[[cite:social]].
   The more lines are added/changed, the higher the probability that the pull request will be
   rejected, but the chance that some quality issue will be introduced is also higher.
   In this case, it is difficult to distinguish if the pull request was rejected because
   of the code quality or because the changes are too big.
* Conclusion
  I analyzed 41576 pull requests from 100 projects written in 5 different
  programming languages (Python, Java, Kotlin, Haskell, C/C++) to study the
  relationship between the code quality and pull request acceptance.  The
  quality of the individual pull requests was measured using static code
  analysis.

  Almost half of the analyzed PRs introduced some code quality issue, and 31 %
  of pull requests fixed some issue. However, data differs significantly between
  the languages (because different static analysis tools were used).  In C/C++
  projects, only 16 % of pull requests introduced some issue, while in Java,
  almost 76 % of PRs. The proportion of accepted pull requests was different for
  each project; however, on average, 76 % of PRs were accepted.

  Several statistic techniques were used to understand if the code quality
  affects PR acceptance. For each language, the $\chi^2$ test of independence
  was performed, and the number of accepted PRs without a code quality issue was always
  higher than expected. However, in all languages, the impact on acceptance was
  only small ($\phi_c \approx 0.1$).

  Multiple classification algorithms were used to predict the pull request acceptance
  using the code quality. However, all of them performed poorly ($AUROC < 0.6$).
  The most problematic was the Haskell language --- all models were no better than a random
  predictor ($AUROC \approx 0.5$).

  The trained models were also used to understand the importance of individual issues.
  Unfortunately, no issue with a significant impact on the PR acceptance was detected.
  All discovered issues have average importance below 1 % (between all models).

  The influence of code quality on time to close a PR was also
  studied. Several regression models were trained to predict this time. However,
  all of the trained models have very high /mean absolute error/: around one month.
  This makes models worthless because most of the PRs are closed within two
  weeks.

  To conclude, the poor code quality seems to have a small negative impact on the
  pull request acceptance. However, there seems to be no effect on the time it
  takes to close a PR.
*** Related work
    Best of my knowledge, there is only one study[[cite:quality]] about the effect of quality flaws
    on the pull request acceptance. Lenarduzzi et al. analyzed 28 well-known Java projects.
    I reused the script they provided for PR classification and also applied similar statistical
    techniques so that my findings could be compared with theirs.
    The $\chi^2$ test of independence yielded similar results (they obtained $\phi_c = 0.12$).
    My classification models have slightly better performance (mean $AUROC$ is higher by $0.023$).
    The difference in performance can be caused by various factors --- project
    selection and the ratio of accepted pull requests (only 53 % of PRs they studied were accepted).
    The code quality was evaluated using the same linter (PMD). However, I also took into
    account issues that were fixed by the PR. Moreover, a different technique was used to identify
    issues that were introduced in the PR (they used diff-files provided by GitHub API).
    Similar to my findings, Lenarduzzi et al. did not identify any particular issues that have a significant
    effect on the acceptance.

    I extended the work of Lenarduzzi with an analysis of four new programming
    languages (Python, Kotlin, Haskell, and C/C++).  I also added the analysis
    of the delivery time of pull requests, and as far as I know, this is the first
    study that researches the relationship between the code quality and the time it
    takes to close a pull request.
*** Future work
    I consider my work complete. However, there is still plenty of
    possibilities for how to improve and expand my work.
    Several improvements can be made to obtain more reliable data for analysis.
    If the pull request is not merged using GitHub, then the PR is incorrectly classified as rejected.
    It is possible to utilize some heuristics that will recognize merging through plain Git utilities.

    For the proper quality evaluation, the linter choice is essential. Each
    linter is focused on some specific set of issues, and this can introduce
    some form of bias. It would be beneficial to use multiple linters for one
    programming language. The linters used for C/C++ and Kotlin are not very
    sophisticated.  However, adding a more advanced linter for C/C++ is
    complicated --- the state-of-the-art linters require information about
    compiler flags. This information cannot always be retrieved automatically
    (from makefiles).  Therefore, a lot of pull requests require manual
    customization.

    Some projects use linters as part of the /continuous integration/ or
    during the build process. Additional research needs to be performed to
    understand if the maintainers of those projects are more strict about the
    code quality, and therefore the effect on the PR acceptance is larger.
* Appendix
  :PROPERTIES:
  :UNNUMBERED: t
  :END:
  \addcontentsline{toc}{chapter}{Appendix}
** Scripts used for analysis
   - *TODO:* fix grammar
   In order to simplify analysis of retrieved data, I created the script (=pr_process.py=) that
   takes multiple JSON files with the data about each individual project and
   converts them into the CSV files. Each row in the CSV file represents some
   pull request. This script also filters the pull requests which are not
   suitable for the analysis --- PRs that do not contains any source code written
   in the primary language or PRs that contained corrupted files (the linter was
   unable to analyze those files).

   The retrieved data about the pull request were subsequently analyzed in order
   to answer my research questions. For the classification (*RQ_2*) was used the Python
   script[fn::https://figshare.com/s/d47b6f238b5c92430dd7] (=pr_classification.py=) provided by Lenurdazzi
   et al.[[cite:quality]].

   I also created the script (=pr_reqression.py=) that runs the regression algorithms on the data in order to answer *RQ_4*.
   This script is written in Python and it uses scikit-learn[fn::https://scikit-learn.org/stable/index.html] library.

   The rest of the analysis was done using the =analysis.R=. This small R program imports the data
   generated by other scripts. This data are then analyzed using various statistical methods.
   Script is also used to plot graphs, create tables and then export them directly into the LaTeX.
** Projects
   #+BEGIN_EXPORT latex
   \begin{table}[htbp]
   \caption{Python projects}
   \centering
   \begingroup\footnotesize
   \input{results/python/projects_summary}
   \endgroup
   \end{table}
   #+END_EXPORT
   #+BEGIN_EXPORT latex
   \begin{table}[htbp]
   \caption{Java projects}
   \centering
   \begingroup\footnotesize
   \input{results/java/projects_summary}
   \endgroup
   \end{table}
   #+END_EXPORT
   #+BEGIN_EXPORT latex
   \begin{table}[htbp]
   \caption{Kotlin projects}
   \centering
   \begingroup\footnotesize
   \input{results/kotlin/projects_summary}
   \endgroup
   \end{table}
   #+END_EXPORT
   #+BEGIN_EXPORT latex
   \begin{table}[htbp]
   \caption{Haskell projects}
   \centering
   \begingroup\footnotesize
   \input{results/haskell/projects_summary}
   \endgroup
   \end{table}
   #+END_EXPORT
   #+BEGIN_EXPORT latex
   \begin{table}[htbp]
   \caption{C/C++ projects}
   \centering
   \begingroup\footnotesize
   \input{results/c_cpp/projects_summary}
   \endgroup
   \end{table}
   #+END_EXPORT
** Issue categories
   #+BEGIN_EXPORT latex
   \begin{table}[htbp]
   \caption{Pylint issue categories}
   \centering
   \begingroup\footnotesize
   \input{results/python/issue_types_summary}
   \endgroup
   \end{table}
   #+END_EXPORT
   #+BEGIN_EXPORT latex
   \begin{table}[htbp]
   \caption{PMD issue categories}
   \centering
   \begingroup\footnotesize
   \input{results/java/issue_types_summary}
   \endgroup
   \end{table}
   #+END_EXPORT
** Model reliability
   - for regression and classification
   - check the Lenarduzzi paper
   - table with recall, precision etc.
** ROC curves
** TODO Create table that compares already performed studies with my thesis
   - Replication Can Improve Prior Results: A GitHub Study of Pull Request Acceptance[[cite:replication]]
     - contains interesting table with factors that influences acceptance
   - Pull Request Decision Explained: An Empirical Overview[[cite:empirical]]
     - also contains interesting table with factors that influences acceptance
* Setup :noexport:
#+LATEX_CLASS: fithesis4
#+LATEX_CLASS_OPTIONS: [digital,oneside,oldtable,nolof,nolot,nocover]
#+LATEX_HEADER: \usepackage{style}
#+BIND: org-latex-title-command ""
#+BIND: org-latex-toc-command ""
#+BIND: org-latex-with-hyperref nil
#+BIND: org-latex-listings minted
#+BIND: org-src-preserve-indentation nil
#+BIND: org-edit-src-content-indentation 0
# Local Variables:
# mode: org
# org-export-allow-bind-keywords: t
# org-latex-classes: '("fithesis4" "\\documentclass{fithesis4}
#                            [NO-DEFAULT-PACKAGES]
#                            [NO-PACKAGES]"
#                        ("\\chapter{%s}" . "\\chapter*{%s}")
#                        ("\\section{%s}" . "\\section*{%s}")
#                        ("\\subsection{%s}" . "\\subsection*{%s}")
#                        ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
#                        ("\\paragraph{%s}" . "\\paragraph*{%s}")
#                        ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# org-latex-pdf-process: ("pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f"
#                         "biber %b"
#                         "pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f"
#                         "pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f")
# display-line-numbers-width: 4
# eval: (org-add-link-type "cite"
#         (defun follow-cite (name))
#         (defun export-cite (path desc format)
#           (if (eq format 'latex)
#           (if (or (not desc) (equal 0 (search "cite:" desc)))
#             (format "~\\cite{%s}" path)
#             (format "~\\cite[%s]{%s}" desc path)))))
# End:
