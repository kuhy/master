#+TITLE: Source Code Quality impact @@latex:\\@@ on Pull Requests acceptance
#+AUTHOR: Ond≈ôej Kuhejda
* Introduction
** Problem statement
   - RQ_1 :: Which code issues are typically introduced by the pull requests?
   - RQ_2 :: Are there some particular issues/code smells that affect the pull request acceptance?
   - RQ_3 :: Is there a relationship between the source code quality and the pull request acceptance?
   - RQ_4 :: Does code quality influence the time it takes to close a pull request?
   - RQ_5 :: Is code quality impact higher in projects that are using some particular programming language?
* Code quality in pull-based development
  The pull-based development model created novel ways how can developers
  interact between each other. Instead of pushing code changes (patches) into
  one central repository, developers can work in more decentralized and
  distributed way. This is mainly done by using distributed version control
  systems such as Git. Git enables developers to clone repositories and thus to
  work independently on projects. Furthermore, the Git's branching model helps
  developers to keep track of repository changes and helps to handle the
  conflicts between the different changes of the same code base.

  To furthermore ease the complicated process of resolving conflicts between
  different changes (of the same code base) and to provide a more user-friendly
  environment for developers, platforms such as GitHub was created. These
  platforms adds new ways how the developers can interact beyond the basic
  functionality of Git:
  - The forks enables to create the server-side copy of the repository.
  - Pull requests (on some platforms called merge requests) enables to merge code directly on the platform.
  - Users can report issues found in the projects; therefore, platform can also serve as a bug-tracking system.
  - The comments can be added to the pull requests and issues in order to build up social interaction between developers.
  - Users can star projects and follow other users, projects, pull requests or issues.

  In this study, I choose to use GitHub as the main source for data
  mining. GitHub is one of the leading platforms that enables pull-based
  collaboration between developers. GitHub hosts huge amount of publicly
  available repositories and GitHub also provides public REST API that can be
  easily leveraged for data mining.

  The aim of this thesis is to obtain large amount of data about GitHub projects
  and analyze the pull request in regard of their code quality. How the code
  quality can be analyzed and how the GitHub platforms contributes to quality of
  the code itself is discussed in the following chapters.
**** TODO cite: An Exploratory Study of the Pull-based Software Development Model
** Code quality
   Code quality is very important aspect of every program --- software with high
   code quality has competitive advantage, is more stable and is also more
   maintainable then software which is poorly written.

   To be able to evaluate the software in regard of its quality, there needs to
   be some way how can be code quality measured. The testing can be used exactly
   for this purpose --- as a tool for measuring the quality of the source code.
   There are multiple ways how can be testing performed. Testing techniques can
   be divided into two categories: static and dynamic testing techniques.

   In order to use dynamic testing techniques on large number of programs, there
   are two large obstacles --- the program needs to be executed and there needs
   to be some inputs (with expected outputs) that can be then used for testing.
   Program execution can be problematic. Some programs needs to be compiled
   before they can be executed; others requires special environment for its
   execution (specific hardware, operating system or shared libraries required
   by the program). Moreover, the most of the programs does not have sets of
   input that can be used for testing. There exists some techniques that can be
   used also without the predefined inputs such as fuzzing, but these techniques
   are usually time-consuming. Because of that, dynamic testing techniques are
   not viable option when dealing with the large number of programs.

   On the other hand, static testing methods suits the analysis of the large
   number of programs better. Static techniques include usage formal and
   informal reviews, walkthroughs and inspections; however, these techniques are
   performed by humans and therefore are not usable for large datasets. Because
   of that, in this thesis, the quality of the given source code is evaluated
   using the tools for automatic static analysis (called linters). Linters are
   used to find defects and code smells in the source code without the need of
   source code's execution.

   There are several categories of issues which can be detected using linters.
   Source code can be checked if it follows a conventions of the given
   programming language. For instance, Python has an official style guide for
   Python code --- PEP 8[fn::https://www.python.org/dev/peps/pep-0008/]. This
   guide defines the conventions that should be followed such as proper
   indentation of the code blocks, maximum line length or naming conventions.

   Furthermore, code can be analyzed against refactoring related checks; for
   instance linter can detect if some part of the code is redundant and
   therefore could be omitted. Linters can also detect actual errors such as
   type mismatches or syntax errors.

   However, it is important to note that not all linters have the same
   capabilities. Number of issues which can be detected by the given linter also
   heavily depends on the programming language of the studied source code. Which
   linters were used for the purposes of this thesis is discussed later in the
   text.
**** TODO cite: https://www.utcluj.ro/media/page_document/78/Foundations%20of%20software%20testing%20-%20ISTQB%20Certification.pdf
** TODO GitHub
   - GitHub issues and code quality
   - Ways to merge code
     - An Exploratory Study of the Pull-based Software Development Model
   - PRs and code review
   - PRs CI/CD and code quality
     - Wait for It: Determinants of Pull Request Evaluation Latency on GitHub[[cite:latency]]
       - CI and latency
     - Trautsch et al.[[cite:pmd]] analyzed several open-source projects in regards to
       usage of static analysis tools.  They found out that incorporating a static
       analysis tool in a build process reduces the defect density.
* Pull request acceptance
  Pull request acceptance is a problem that has been studied multiple
  times. Several surveys were performed in order to understand why pull requests
  are being rejected.

  Gousios et al.[[cite:integrator]] surveyed hundreds of integrators to find out
  their reasons behind the PR rejection. Code quality was stated as the main
  reason by most of the integrators; code style was in the second place.
  Factors that integrators examine the most when evaluating the code quality are
  style conformance and test coverage.

  Kononenko et al.[[cite:shopify]] performed a study of an open-source project
  called /Shopify/; they manually analyzed PR's and also surveyed /Shopify/
  developers. They found out that developers associate the quality of PR with
  the quality of its description and with the revertability and complexity of
  the PR.

  The reasons why contributors abandon their PRs were also
  studied[[cite:abandonment]]. The reason number one was the ``Lack of answers from
  integrators.''; moreover, the ``Lack of time'' and the ``Pull request is
  obsolete'' was also often stated as the main reason.

  Even though the different open-source communities solve the problem of pull
  request acceptance in a different manner, three main governance styles can be
  identified --- protective, equitable, lenient. Protective governance style
  values trust in the contributor-maintainer relationship. The equitable
  governance style tries to be unbiased towards the contributors, and the
  lenient style prioritizes the growth and openness of the community[[cite:foss]].
  Each style focuses on different aspects of PR. Tsay et al.[[cite:social]]
  identified the following levels of social and technical factors that influence
  the acceptance of the PR --- /repository level/, /submitter level/, and the
  /pull request level/.
** Repository level
   The /repository level/ is interested in the aspects of the repository itself,
   such as the repository age, number of collaborators, or number of stars on
   the GitHub.

   For instance, the programming language used in the project also influences
   the acceptance of the PRs. Pull requests containing Java, JavaScript, or C++
   code have a smaller chance to be accepted than PRs containing the code
   written in Go or Scala[[cite:factors]].

   Furthermore, older projects and projects with a large team have a
   significantly lower acceptance rate[[cite:social]].

   The popularity of the project also influences the acceptance rate ---
   projects with more stars have more rejected PRs[[cite:social]].
** Submitter level
   The /submitter level/ is concerned about the submitter's status in the
   general community and his status in the project itself. There are several
   parameters that can be considered when evaluating the submitter's status.

   PRs of submitters with higher social connection to the project have a higher
   probability of being accepted[[cite:social]].

   Submitter status in the general community plays an important role in PR
   acceptance. If the submitter is also a project collaborator, the likelihood
   that the PR will be accepted increases by 63.3%[[cite:social]].

   Moreover, users that contributed to a larger number of projects have a higher
   chance that their PR will be accepted[[cite:npm2]].

   The gender of the submitter is another factor that plays a role in PR
   acceptance. A study showed that woman's PR are accepted more often, but only
   when they are not identifiable as a woman[[cite:gender]].

   Personality traits also influence PR acceptance. The /IBM Watson Personality
   Insights/ were used to obtain the personality traits of the PR submitters by
   analyzing the user's comments. These traits were then used to study PR
   acceptance. It has been shown that conscientiousness, neuroticism, and
   extroversion are traits that have positive effects on PR acceptance. The
   chance that PR will be accepted is also higher when the submitter and closer
   have different personalities[[cite:personality]].
** Pull request level
   The /pull request level/ is interested in the data that are connected to the
   PR itself.  For instance, on the /PR level/, one can study if there is
   a correlation between PR acceptance and the number of GitHub comments in
   the PR. Another parameter that can be used is ``Number of Files Changed'' or
   ``Number of Commits''.

   One of the factors that negatively influence the acceptance rate is the
   already mentioned number of commits in the pull request. The high number of
   commits decreases the probability of acceptance. On the other hand, PR's with
   only one commit are exceptions --- they have a smaller chance to be accepted
   than pull requests which contain two commits[[cite:npm2]].

   Another observation is that more discussed PR's has a smaller chance to be
   accepted[[cite:social]].  Another study did not find a large difference between
   accepted and rejected PR's based on the number of comments but found that
   discussions in rejected PR's have a longer duration[[cite:discussion]].

   Proper testing is the crucial part of every project, and therefore it also
   influences the pull request acceptance.  PR's including more tests have a
   higher chance to be accepted, and an increasing number of changed lines
   decreases the likelihood of PR acceptance[[cite:social]].

   Testing plays a significant role in discovering bugs and therefore leads to
   higher code quality. On the other hand, many test cases do not have
   to mean that code has a high quality. The code quality is an essential
   factor on the /pull request level/, therefore, is this study's main interest.
   Works that are also interested in the code quality and the pull
   request acceptance are examined in the following chapter.

   Another factor that is closely tied to code quality is the code style.
   This factor has a small (but not negligible) negative effect on
   acceptance. This means that PRs with larger code style inconsistency
   (with the codebase) have a smaller chance of being accepted[[cite:style]].
** TODO Code quality
   Although most integrators view code quality as the most important factor
   regarding PR acceptance, to the best of my knowledge, only one study was
   performed to discover whether there is a connection between the PR's
   acceptance and its quality.
   - Does code quality affect pull request acceptance?[[cite:quality]]
** TODO Unsorted
   - study ``Influence of Social and Technical Factors''[[cite:social]] was replicated[[cite:personality]]
   - Replication Can Improve Prior Results: A GitHub Study of Pull Request Acceptance[[cite:replication]]
     - contains interesting table with factors that influences acceptance
   - Pull Request Decision Explained: An Empirical Overview[[cite:empirical]]
     - also contains interesting table with factors that influences acceptance
   - An Exploratory Study of the Pull-Based Software Development Model[[cite:explaratory]]
   - Which Pull Requests Get Accepted and Why? A study of popular NPM Packages[[cite:npm]]
   - Rejection Factors of Pull Requests Filed by Core Team Developers in Software Projects with High Acceptance Rates[[cite:developers]]
   - Pull Request Prioritization Algorithm based on Acceptance and Response Probability[[cite:prioritization]]
** TODO Create table that compares already performed studies with my thesis
* Data mining
  *TODO*: update graph
  #+BEGIN_EXPORT latex
  \begin{figure}[htb]\centering
  \begin{tikzpicture}
  \node (n1) [align=center] {Project\\name};
  \node (n2) [box, above=of n1, align=center] {\texttt{gh\_info.py}};
  \node (n3) [cloud, draw, above=of n2, align=center] {GHTorrent\\database};
  \node (n4) [right=of n2, align=center] {Pull requests\\information};
  \node (n5) [box, right=of n4] {\texttt{git-contrast}};
  \node (n6) [cloud, draw, above=of n5] {Linters};
  \node (n7) [below=of n5, align=center] {Pull requests\\code quality};
  \node (n8) [below=of n4, align=center] {Project\\information};

  \node [container, fit=(n1)(n2)(n3)(n4)(n5)(n6)(n7)(n8)] {};
  \node [below left, inner sep=3mm] at (current bounding box.north east) {\texttt{pr\_quality.py}};

  \draw[->] (n1) to (n2);
  \draw[<->] (n2) to (n3);
  \draw[->] (n2) to (n4);
  \draw[->] (n4) to (n5);
  \draw[<->] (n5) to (n6);
  \draw[->] (n5) to (n7);
  \draw[->] (n2) to (n8);
  \end{tikzpicture}
  \caption{The \texttt{pr\_quality.py} workflow}\label{workflow}
  \end{figure}
  #+END_EXPORT
  Information about the pull requests are retrieved using the =pr_quality.py=
  script. This scripts takes names of the projects that will be analyzed as the
  input and it outputs the JSON files containing the requested data. This script
  uses internally two other scripts --- =gh_info.py= and =git-contrast=.

  =gh_info.py= is responsible for querying the GHTorrent database in order to
  obtain data about the projects. The GHTorrent database is an offline mirror of
  data offered through the Github REST API. =gh_info.py= returns a JSON file
  with the information about the project such as number of stars, number of
  contributors or information about pull requests and their commits.

  However, the Github REST API lacks the information about the code quality of
  the pull requests. This is where the =git-contrast= comes into the play.
  =git-contrast= is the command-line application which analyzes the code quality
  of the given pull request using the external linters. This application is
  further discussed in the following sections.

  *TODO*: mention that data from REST API are not complete (GH API limit)
** GHTorrent database
   As stated before, the script called =gh_info.py= uses the GHTorrent database
   in order to retrieve GitHub data. GitHub REST API can be leveraged to obtain
   many interesting factors which can possibly influence the acceptance of pull
   requests. All the data that are obtained using the =gh_info.py= are listed
   in the following table:
   #+CAPTION: Data retrieved from the GHTorrent
   #+ATTR_LaTeX: :align |llc|
   |--------------------+---------------------------+-----------|
   | Level              | Variable                  | Factor    |
   |--------------------+---------------------------+-----------|
   |--------------------+---------------------------+-----------|
   | Repository level   | Project name              | \ding{55} |
   |                    | Programming language      | \ding{51} |
   |                    | Time of creation          | \ding{51} |
   |                    | Number of forks           | \ding{51} |
   |                    | Number of commits         | \ding{51} |
   |                    | Number of project members | \ding{51} |
   |                    | Number of watchers        | \ding{51} |
   |--------------------+---------------------------+-----------|
   | Submitter level    | Username                  | \ding{55} |
   |                    | Number of followers       | \ding{51} |
   |                    | Status in the project     | \ding{51} |
   |--------------------+---------------------------+-----------|
   | Pull request level | Pull request ID           | \ding{55} |
   |                    | Is PR accepted?           | \ding{51} |
   |                    | Time opened               | \ding{51} |
   |                    | Head repository           | \ding{55} |
   |                    | Head commit               | \ding{55} |
   |                    | Base commit               | \ding{55} |
   |                    | Number of commits         | \ding{51} |
   |                    | Number of comments        | \ding{51} |
   |--------------------+---------------------------+-----------|

   Variables marked with ticks (\ding{51}) are factors that can possibly
   influence code quality and they can be used for pull request acceptance
   analysis. Other variables (\ding{55}) are not meant to be used as an part of
   an data analysis itself, but are kept here for better orientation; and some
   of them are later used by the =git-contrast= tool (in order to pull the
   commits which will be subsequently analyzed by linters).
** =git-contrast=
   =git-contrast= is the command line application that I implemented in order to
   be able to analyze the code quality of the given pull request. =git-contrast=
   expects two commit hashes on the input and returns the information about the
   change in code quality between these commits on the output. This is done by
   running the linter on the files in the state of the first commit and then in
   the state of the second commit. The number of found code quality issues is
   then written to the standard output.

   To measure the change of the quality in the pull request, we simple run the
   =git-contrast= on the ``head commit'' and the ``base commit'' of the given
   pull request. =git-contrast= supports several linters; which linter will be
   used is determined by the file extension of the tested file. Linters that are
   supported by =git-contrast= are listed in the following table:
   #+CAPTION: Linters supported by the =git-contrast=
   #+ATTR_LaTeX: :align |lll|
   |------------+-----------------------+-----------------------|
   | Linter     | Programming languages | File extensions       |
   |------------+-----------------------+-----------------------|
   |------------+-----------------------+-----------------------|
   | *OCLint*   | C/C++                 | =.c=, =.cpp= and =.h= |
   | *HLint*    | Haskell               | =.hs=                 |
   | *ktlint*   | Kotlin                | =.kt= and =.kts=      |
   | *PMD*      | Java                  | =.java=               |
   | *Pylint*   | Python                | =.py=                 |
   |------------+-----------------------+-----------------------|

   The most problematic was to statically analyze the C/C++ source files because
   some linters also need the information how the source code should be
   compiled. Luckily, this information can be usually automatically obtained
   from the makefiles. Another problem is the speed. At first, I was using the
   Cppcheck linter for the static analysis of C/C++ but I was forced to switch
   to the OCLint in order to shrink the total execution time of the static
   analysis.
** TODO Projects selection
   Criteria (data from 2019-06-01):
   - is in the top 150 most favorite projects written in the given language
   - 200+ pull requests and less then 5000
   - https://github.com/EvanLi/Github-Ranking
   - at least 85 % of files are source files written in the given language
   - project is a program or program collection (not a book with the script etc.)

   - https://dl.acm.org/doi/abs/10.1145/2597073.2597122
   - https://dl.acm.org/doi/abs/10.1145/3379597.3387489
   - https://zenodo.org/record/3858046
   - https://github.com/XLipcak/rev-rec
   - https://ghtorrent.org/
     - https://github.com/gousiosg/pullreqs
     - How can I cite this work? (on the web)
   - Kalliamvakou et al. noted that data about PR's mined from GitHub are not always reliable,
     because PR can be also merged using several different approaches.
     - https://dl.acm.org/doi/10.1145/2597073.2597074
     - [[cite:explaratory]]
** TODO Computational resources
* Data analysis
  In order to simplify analysis of retrieved data, I created the script that
  takes multiple JSON files with the data about each individual project and
  converts them into the CSV files. Each row in the CSV file represents some
  pull request. This script also filters the pull requests which are not
  suitable for the analysis --- PRs that do not contains any source code written
  in the primary language or PRs that contained corrupted files (the linter was
  unable to analyze those files).

  The retrieved data about the pull request were subsequently analyzed in order
  to answer my research questions.  For this analysis was used the Python
  script[fn::https://figshare.com/s/d47b6f238b5c92430dd7] provided by Lenurdazzi
  et al. (this script was used for machine learning classification methods) and
  also my script written in R. The statistical methods that were used are
  discussed in the following paragraphs:

  Research questions 1 to 4 were analyzed separately for each programming language.

  *TODO:* At first, for *RQ_1*, I summarized the retrieved data for each project
  --- I counted how many suitable pull requests were analyzed and
  how many of them are accepted/rejected. Then for each issue individually I
  computed how many accepted/rejected pull request introduced/fixed this issue,
  how many times this issue occurred in some pull request etc.

  - https://www.scribbr.com/statistics/statistical-tests/

  *RQ_2*:
  - classification (machine learning)
  - machine learning while taking into account only introduced issues vs quality change
  - correlation matrix

  *RQ_3*:
  - PCA scatterplot
  - contingency matrices
  - contingency matrices for PR's that contain only modified source code files vs for all of them
  - contingency matrices separately for each project
  - ROC curves and AUCs

  *RQ_4*:
  - regression (machine learning)
  - correlation matrix

  *RQ_5*:
  - Compare results from previous steps.
    - Statistically compare the parameters obtained for each programming language.
     (check if the pull requests from different languages and retrieved parameters
     follow the same distributions)
  - What is the effect of the programming language on the acceptance and
    time it takes to close a pull request?
    - ANOVA
    - classification (machine learning)
* Evaluation
  *TODO*
** Python
   In order to analyze the influence of code quality on the pull request
   acceptance, the following projects from the Python ecosystem were selected:
   #+BEGIN_EXPORT latex
   \begin{table}[htbp]
   \caption{Python projects}
   \centering
   \begingroup\footnotesize
   \input{data/python/projects_summary}
   \endgroup
   \end{table}
   #+END_EXPORT

   In total, 9452 pull requests were analyzed and 73 % of these PRs was accepted.
   Pull request were more accepted in less popular projects.

   At average, one pull request introduced 5.36 issues and fixed 2.44 issues;
   accepted pull request introduced 4.62 and fixed 1.99 issues, and rejected
   pull request introduced 7.86 issues and fixed 4.43 on average.
   5 % trimmed mean was used to compute these values.

   The Pylint, which was used to lint those projects, classifies issues into the
   following categories:
   #+BEGIN_EXPORT latex
   \begin{table}[htbp]
   \caption{Pylint issue categories}
   \centering
   \begingroup\footnotesize
   \input{data/python/issue_types_summary}
   \endgroup
   \end{table}
   #+END_EXPORT

   In the analyzed pull requests, Pylint detected 222 different issues.

   The list of issues that was fixed/introduced in the largest number of pull
   request was dominated by the conventions.  The convention that was
   fixed/introduced in the largest number of pull requests is
   =missing-function-docstring= (in 37 % of PRs); also conventions
   =invalid-name=, =line-too-long= and =consider-using-f-string= were
   fixed/introduced in over then 20 % of pull requests. There were 15 issues
   that were fixed/introduced in more then 10 % of PRs and 72 issues were in
   over 1 % of PRs (out of the 222 issues which were found in the pull
   requests).  There were 9 issues which was present in the analyzed pull
   requests but did not influence their quality (number of these issues was not
   changed by any pull request). 13 issues were introduced/fixed in only one
   pull request and 10 of them are issues classified as errors. The most common
   error is =import-error= (24 % of PRs); however, I suspect that there will be
   a lot of false positives which aroused due to linting in the isolated
   environment. 60 issues were fixed in more PR's then they were introduced.
   They are 24 more PRs that fixed the warning =super-init-not-called= than the
   PR's that introduced it.
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth + 2cm}{!}{%
     \input{data/python/issues_appeared_in}
   }
   \caption{Pylint issues and \% of PRs which fixed/introduced them}
   \end{figure}
   #+END_EXPORT

   The most important Pylint issue in regards of the PR acceptance is the
   =syntax-error=.  XGBoost classifier gives this error the 1 %
   importance. However, other classifiers consider this error less important.
   The average importance of the =syntax-error= is only 0.4 %.
   *TODO*: investigate the =syntax-error= in more detail
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \input{data/python/issue_importance}
   \caption{Ten most important Pylint issues}
   \end{figure}
   #+END_EXPORT

   In order to visualize the difference in quality between accepted and rejected PRs, I created PCA scatter plot:
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \input{data/python/acceptance_pca}
   \caption{PCA scatter plot}
   \end{figure}
   #+END_EXPORT
   In the PCA scatter plot, there is no visible difference between rejected and accepted pull requests.

   To understand if the presence of some issue in the PR influences its acceptance, I created contingency matrices
   and performed chi-square test of independence:
   #+BEGIN_EXPORT latex
   \begin{figure}[H]\centering
   \resizebox{\textwidth}{!}{%
     \input{data/python/acceptance_ct}
   }
   \caption{Relation between presence of issue and PR acceptance}\label{Python_CT}
   \end{figure}
   #+END_EXPORT

   As can be seen in Figure\nbsp{}\ref{Python_CT}, the observed number of rejected pull requests which contained some defected is
   higher then expected. The p-value of chi-square test is less then \num{2.2e-16} and therefore, the hypothesis that presence of
   some issue and PR acceptance are independent is rejected on significance level $\alpha = 0.05$.

   Pull request that are adding or removing some files has large influence on
   code quality. If the number of removed/added files has large impact on PR
   acceptance, then it can be a large thread to validity of the independence
   test.  The pull request acceptance can be also influenced by quality of files
   which were not linted (was written in non-primary language).  To eliminate
   the risk that the test was influenced, same test was performed on pull
   requests that only modified some source files and these files were written in the
   primary language. The resulting p-value was \num{5.548e-10} and therefore the
   fact that PRs introduced some code quality issues influences their acceptance.
** Threads to validity
   - PR recognition (rejected PRs can be merged using another way)
   - modified files in the PR's that are not written in the primary programming language can influence acceptance
   - chosen projects
   - PR's filtering
     - linter errors
     - limit of 500 PRs
     - limited execution time for =git-contrast=
     - not available PR's/repositories
   - false positives from Linters (=import-error=, =relative-beyond-top-level=)
* Conclusion
** Future work
* Appendix
* Setup :noexport:
#+LATEX_CLASS: fithesis4
#+LATEX_CLASS_OPTIONS: [digital,oneside,oldtable,nolof,nolot,nocover]
#+LATEX_HEADER: \usepackage{style}
#+BIND: org-latex-title-command ""
#+BIND: org-latex-toc-command ""
#+BIND: org-latex-with-hyperref nil
#+BIND: org-latex-listings minted
#+BIND: org-src-preserve-indentation nil
#+BIND: org-edit-src-content-indentation 0
# Local Variables:
# mode: org
# org-export-allow-bind-keywords: t
# org-latex-classes: '("fithesis4" "\\documentclass{fithesis4}
#                            [NO-DEFAULT-PACKAGES]
#                            [NO-PACKAGES]"
#                        ("\\chapter{%s}" . "\\chapter*{%s}")
#                        ("\\section{%s}" . "\\section*{%s}")
#                        ("\\subsection{%s}" . "\\subsection*{%s}")
#                        ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
#                        ("\\paragraph{%s}" . "\\paragraph*{%s}")
#                        ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# org-latex-pdf-process: ("pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f"
#                         "biber %b"
#                         "pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f"
#                         "pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f")
# display-line-numbers-width: 4
# eval: (org-add-link-type "cite"
#         (defun follow-cite (name))
#         (defun export-cite (path desc format)
#           (if (eq format 'latex)
#           (if (or (not desc) (equal 0 (search "cite:" desc)))
#             (format "~\\cite{%s}" path)
#             (format "~\\cite[%s]{%s}" desc path)))))
# End:
