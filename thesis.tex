% Created 2022-05-12 Thu 01:54
% Intended LaTeX compiler: pdflatex
\documentclass[digital,oneside,oldtable,nolof,nolot,nocover]{fithesis4}
                           

\usepackage{style}
\setcounter{secnumdepth}{2}
\author{Ond≈ôej Kuhejda}
\date{\today}
\title{Source Code Quality Impact \\ on Pull Requests Acceptance}
\begin{document}

\chapter{Introduction}
\label{sec:org9370293}
\emph{Does code quality influence the acceptance of pull requests?} Although many
project maintainers view code quality as the most important factor regarding
pull request acceptance~\cite{integrator}, a recent study by Lenarduzzi et al.
shows that the presence of quality flaws in the code does not influence the
acceptance or rejection of pull requests~\cite{quality}.

To the best of my knowledge, study by Lenarduzzi et al. is the only one that
investigated if quality issues affect the acceptance of pull requests. They
analyzed 28 well-known Java projects and applied several statistical
techniques to find the relation between code quality and pull request
acceptance. The quality of the pull requests was evaluated using the
open-source tool called PMD. This tool is able to perform static analysis of
the code and detect common programming flaws, such as code-smells,
anti-patterns, or code-style violations. Traditional statistical techniques
did not find any connection between the code quality and pull request
acceptance. Because of that, they trained several machine learning models to
predict the acceptance based on the quality issues found in the code. However,
machine learning models yielded similar results as traditional techniques.

Unfortunately, the study performed by Lenarduzzi et al., while an extensive
study, analyzed only projects written in Java. Moreover, 22 out of 28 analyzed
projects were from the Apache Software Foundation. This is the major threat to
the generalizability of their findings. To address these shortcomings, I
analyzed one hundred projects from five different programming languages ---
Python, Java, Kotlin, Haskell, and C/C++ (twenty projects per language). For
each programming language, a different tool for static analysis was used to
evaluate the code quality. I applied similar techniques as Lenarduzzi et
al. to analyze the relationship between the code quality and pull request
acceptance. Furthermore, the regression models were created to predict the
time required to close a pull request using the code quality. Subsequently,
the retrieved results were compared between individual languages.
The research questions set for this thesis are the following:
\begin{description}
\item[{RQ\textsubscript{1}}] Which code issues are typically introduced by the pull requests?
\item[{RQ\textsubscript{2}}] Are there some particular quality flaws that affect the acceptance of the pull request?
\item[{RQ\textsubscript{3}}] Is there a relationship between the source code quality and the pull request acceptance?
\item[{RQ\textsubscript{4}}] Does code quality influence the time required to close a pull request?
\item[{RQ\textsubscript{5}}] Is code quality impact higher in projects that are using some particular programming language?
\end{description}

This thesis first discusses code quality and its link to the pull-based
development model (Chapter~\ref{sec:org350eef2}).
After that, the various factors that influence the pull request acceptance are
mentioned (Chapter~\ref{sec:org34db84a}). The next part of the thesis
is dedicated to the research design. At first, I state how the data about the
project was retrieved and which methods were used to evaluate the code quality
of individual pull requests (Chapter~\ref{sec:orgb654080}). Then, the used
statistical methods are introduced (Chapter~\ref{sec:org82f3c27}). Furthermore,
the retrieved results are evaluated separately for each programming language
(Chapter~\ref{sec:org2c89279}). Towards the end, results are compared between
languages, and potential threats to validity are discussed. Finally, I
summarize my findings and compare them to the findings of Lenarduzzi et al.
(Chapter~\ref{sec:orge3d917e}). Moreover, the potential extensions of my work are
proposed.
\chapter{Code quality in pull-based development}
\label{sec:org350eef2}
The pull-based development model created novel ways how developers can
interact with each other. Instead of pushing code changes (patches) into
one central repository, developers can work in a more decentralized and
distributed way. This is mainly done by using distributed version control
systems such as Git. Git enables developers to clone repositories and thus
work independently on projects. Furthermore, Git's branching model helps
developers to keep track of repository changes and helps to handle the
conflicts between the different changes of the same code base~\cite{exploratory}.

To furthermore ease the complicated process of resolving conflicts between
different changes (of the same code base) and to provide a more user-friendly
environment for developers, platforms such as GitHub were created. These
platforms add new ways how the developers can interact beyond the basic
functionality of Git:
\begin{itemize}
\item The forks enable the creation of the server-side copy of the repository.
\item Pull requests\footnote{pull request is commonly abbreviated as PR}
(on some platforms called merge requests) enables to merge code directly on the platform.
\item Users can report issues found in the projects; therefore, the platform can also serve as a bug-tracking system.
\item The comments can be added to the pull requests and issues in order to build up social interaction between developers.
\item Users can star projects and follow other users, projects, pull requests, or issues.
\end{itemize}

In this study, I choose to use GitHub as the main source for data
mining. GitHub is one of the leading platforms that enables pull-based
collaboration between developers. GitHub hosts a huge amount of publicly
available repositories and GitHub also provides public REST API that can be
easily leveraged for data mining.

The aim of this thesis is to obtain a large amount of data about GitHub projects
and analyze the pull request in regard to their code quality. How the code
quality can be analyzed and how the GitHub platforms contribute to the quality of
the code itself is discussed in the following chapters.
\section{Measuring code quality}
\label{sec:org1534815}
Code quality is a very important aspect of every program --- software with high
code quality has a competitive advantage, is more stable, and is also more
maintainable than software that is poorly written.

To be able to evaluate the software in regard to its quality, there needs to
be some way how the code quality can measured. The testing can be used exactly
for this purpose --- as a tool for measuring the quality of the source code.
There are multiple ways how can be testing performed. Testing techniques can
be divided into two categories: static and dynamic testing techniques~\cite{istqb}.

In order to use dynamic testing techniques on a large number of programs, there
are two large obstacles --- the program needs to be executed, and there need
to be some inputs (with expected outputs) that can then be used for testing.
Program execution can be problematic. Some programs need to be compiled
before they can be executed; others require a special environment for their
execution (specific hardware, operating system, or shared libraries required
by the program). Moreover, most of the programs do not have predefined sets of
input that can be used for testing. There exist some techniques that can also be
used without the predefined inputs, such as fuzzing, but these techniques
are usually time-consuming. Because of that, dynamic testing techniques are
not a viable option when dealing with a large number of programs.

On the other hand, static testing methods suit the analysis of a large
number of programs better. Static techniques encompass the usage of formal and
informal reviews, walkthroughs, and inspections; however, these techniques are
performed by humans and therefore are not viable for large datasets. Because
of that, in this thesis, the quality of the given source code is evaluated
using the tools for automatic static analysis (called linters). Linters are
used to find defects and code smells in the source code without the need for
the source code's execution.

The ISO/IEC 25010~\cite{ISO25010} defines several quality characteristics which can be identified in the software.
I will now discuss these characteristics in the context of static analysis:
\begin{description}
\item[{Performance efficiency}] evaluates if the application is using the optimal amount of resources.
The static analysis can help to create a faster code.  For instance, some
linters are able to detect constructs/functions that are ineffective and
consume more resources than required.
\item[{Usability}] is the degree to which the software is easy to use. This quality is often evaluated through
\emph{usability testing}. On the other hand, there are some properties that can be checked via static analysis,
such as proper documentation of public interfaces, which contributes to the application's learnability.
\item[{Reliability}] defines how stable and fault-tolerant the software is.
Static analysis can unravel error-prone constructs and multi-threading issues
(that negatively influence stability) and ensure that exception handling is properly implemented.
\item[{Security}] is concerned with the confidentiality, integrity, and authenticity of the software.
Linters can detect several security-related issues in the source code, such as the use of vulnerable functions
or use of the hard-coded values for cryptographic operations.
\item[{Maintainability}] is the ease with which can be application modified.
Static analysis can help to ensure that source code is clean and
understandable.  Source code can be checked if it follows the conventions of
the given programming language. For instance, Python has an official style
guide for Python code --- PEP 8\footnote{\url{https://www.python.org/dev/peps/pep-0008/}}.
This guide defines the conventions that should be followed, such as proper
indentation of the code blocks, maximum line length, or naming conventions.
Furthermore, code can be analyzed if the software is properly designed and
does not use complicated constructs; for instance linter can detect if some
part of the code is redundant, complicated, or too coupled.
\item[{Portability}] is the ability to execute software on multiple platforms.
Some linters are capable of detecting functions and data types that are not portable.
\end{description}

However, it is important to note that not all linters have the same
capabilities. Issues that can be detected by the given linter heavily
depend on the used programming language (some quality issues are
language-specific). Which linters were used for the purposes of this thesis
is discussed later in the text.

The code issues (the number of their occurrences) identified by linters were
used as a metric to evaluate the code quality of the given pull request.  The
same approach was used by Lenarduzzi et al.~\cite{quality} during the evaluation
of the pull requests code quality.
\section{GitHub and code quality}
\label{sec:org70d0fa1}
GitHub brings many features that may potentially improve code quality.
GitHub has a built-in bug tracker which can be used to report issues found in the code.
Because the issues can be reported by users outside of the core development team,
the code quality issues can be detected earlier and more efficiently. Bug trackers
also enable prioritization of issues which helps to decide which problems need attention first.

Moreover, GitHub enables the creation of pull requests --- a mechanism by which the developers can propose changes to
the code base. When the pull request is submitted, the maintainers of the repository decide if the changes
will be applied (merged) or not (rejected). Quality can be one factor that can influence this decision.
The versatility of Git enables pull requests to be merged in various ways~\cite{exploratory}: through GitHub facilities,
using Git merge, or by committing the patch.

One of the pull requests advantages is the integration with the code review functionality. Maintainers of the projects
can review the code to improve internal code quality and maintainability.

GitHub provides CI/CD\footnote{continuous integration/continuous delivery}
functionality via GitHub Actions\footnote{\url{https://github.com/features/actions}}.
This enables to automatically run static analysis or automated tests whenever
some predefined event occurs, such as creating a new pull request.  Another
possibility is to add a linter directly to the build process and then trigger
the build using the GitHub Actions.  Trautsch et al.~\cite{pmd} analyzed several
open-source projects in regards to the usage of static analysis tools.  They
found out that incorporating a static analysis tool in a build process
reduces the defect density.
\chapter{Pull request acceptance}
\label{sec:org34db84a}
Pull request acceptance is a problem that has been studied multiple
times. Several surveys were performed in order to understand why pull requests
are being rejected.

Gousios et al.~\cite{integrator} surveyed hundreds of integrators to find out
their reasons behind the PR rejection. Code quality was stated as the main
reason by most of the integrators; code style was in the second place.
Factors that integrators examine the most when evaluating the code quality are
style conformance and test coverage.

Kononenko et al.~\cite{shopify} performed a study of an open-source project
called \emph{Shopify}; they manually analyzed PR's and also surveyed \emph{Shopify}
developers. They found out that developers associate the quality of PR with
the quality of its description and with the revertability and complexity of
the PR.

The reasons why contributors abandon their PRs were also
studied~\cite{abandonment}. Reason number one was the ``Lack of answers from
integrators.''; moreover, the ``Lack of time'' and the ``Pull request is
obsolete'' was also often stated as the main reason.

Even though the different open-source communities can approach the pull request acceptance in
a different manner, three main governance styles can be
identified --- protective, equitable, and lenient. The protective governance style
values trust in the contributor-maintainer relationship. The equitable
governance style tries to be unbiased towards the contributors, and the
lenient style prioritizes the growth and openness of the community~\cite{foss}.
Each style focuses on different aspects of PR. Tsay et al.~\cite{social}
identified the following levels of social and technical factors that influence
the acceptance of the PR --- \emph{repository level}, \emph{submitter level}, and the
\emph{pull request level}.
\section{Repository level}
\label{sec:orgac454c1}
The \emph{repository level} is interested in the aspects of the repository itself,
such as the repository age, number of collaborators, or number of stars on
the GitHub.

For instance, the programming language used in the project also influences
the acceptance of the PRs. Pull requests containing Java, JavaScript, or C++
code have a smaller chance of being accepted than PRs containing the code
written in Go or Scala~\cite{factors}.

Furthermore, older projects and projects with a large team have a
significantly lower acceptance rate~\cite{social}.

The popularity of the project also influences the acceptance rate ---
projects with more stars have more rejected PRs~\cite{social}.
\section{Submitter level}
\label{sec:org69a3262}
The \emph{submitter level} is concerned about the submitter's status in the
general community and his status in the project itself. There are several
parameters that can be considered when evaluating the submitter's status.

PRs of submitters with higher social connection to the project have a higher
probability of being accepted~\cite{social}.

Submitter status in the general community plays an important role in PR
acceptance. If the submitter is also a project collaborator, the likelihood
that the PR will be accepted increases by 63.3\%~\cite{social}.

Moreover, users that contributed to a larger number of projects have a higher
chance that their PR will be accepted~\cite{npm2}. The acceptance of the new
pull request also correlates with the acceptance of other older pull requests
created by the same submitter~\cite{npm}~\cite{replication}. Furthermore,
the first pull requests of users are more likely to be rejected~\cite{developers}.

The gender of the submitter is another factor that plays a role in PR
acceptance. A study showed that woman's PR are accepted more often, but only
when they are not identifiable as a woman~\cite{gender}.

Personality traits also influence PR acceptance. The \emph{IBM Watson Personality
Insights} were used to obtain the personality traits of the PR submitters by
analyzing the user's comments. These traits were then used to study PR
acceptance. It has been shown that conscientiousness, neuroticism, and
extroversion are traits that have positive effects on PR acceptance. The
chance that PR will be accepted is also higher when the submitter and closer
have different personalities~\cite{personality}.
\section{Pull request level}
\label{sec:org57a3a02}
The \emph{pull request level} is interested in the data about
PR itself.  For instance, on the \emph{PR level}, one can study if there is
a correlation between PR acceptance and the number of GitHub comments in
the PR.

One of the factors that negatively influence the acceptance rate is the
number of commits in the pull request. The high number of
commits decreases the probability of acceptance. On the other hand, PRs with
only one commit are exceptions --- they have a smaller chance of being accepted
than pull requests which contain two commits~\cite{npm2}.

Another observation is that more discussed PRs have a smaller chance of being
accepted~\cite{social}.  Another study did not find a large difference between
accepted and rejected PRs based on the number of comments but found that
discussions in rejected PRs have a longer duration~\cite{discussion}.
Moreover, the increasing number of changed lines decreases the
likelihood of PR acceptance~\cite{social}.

The code quality is an essential factor on the \emph{pull request level}, and it
is this study's main interest.  The code quality as the acceptance factor is
examined in the following subchapter.
\subsection*{Code quality}
\label{sec:org06b1ef1}
One of the instruments that ensure that the code has high quality is testing.
Proper testing is a crucial part of every project. Testing plays a
significant role in discovering bugs and therefore leads to higher code
quality.  One study found that PRs, including more tests, have a higher
chance of being accepted~\cite{social}. However, another study yields no
relation between acceptance and test inclusion~\cite{exploratory}.

Another factor that is closely tied to code quality is the code style.
Proper and consistent code style increases the maintainability of the
software.  The code style inconsistency has a small (but not negligible)
negative effect on acceptance. PRs with larger code style inconsistency
(with the codebase) have a smaller chance of being accepted.  Code style
inconsistency also negatively influences the time required to close a
PR~\cite{style}.

Although many integrators view code quality as the most important factor
regarding PR acceptance~\cite{integrator}, to the best of my knowledge, only
one study~\cite{quality} was performed to discover whether there is a
connection between the PR's acceptance and the quality flaws found in the
code (taking into account more complicated aspects than code style or test
inclusion).

Lenarduzzi et al.~\cite{quality} analyzed 28 open-source projects. The results show that
there is no significant connection between code quality and PR acceptance.
The key difference (from my thesis) is that they analyzed only projects written
in Java. Furthermore, my thesis investigates the connection between the time to close a
PR and the PR quality. Further comparison is at the end of the thesis.
\chapter{Data mining}
\label{sec:orgb654080}
\begin{figure}[htb]\centering
\begin{tikzpicture}
\node (n1) [align=center] {Project name};

\node (n2) [box, above=2cm of n1, align=center] {\texttt{gh\_db.py}\\(\texttt{gh\_rest.py})};
\node (n3) [cloud, draw, above=of n2, align=center, inner sep=-3mm] {GHTorrent database\\(GitHub REST API)};
\node (c1) [container, fit=(n2)(n3)] {};

\node (n4) [right=2cm of n2, align=center, margin] {Pull requests\\information};

\node (n5) [box, right=of n4] {\texttt{git-contrast}};
\node (n6) [cloud, draw, above=of n5] {Linters};
\node (c2) [container, fit=(n5)(n6)] {};

\node (c3) [container, thick, fit=(c1)(c2)] {};

\node (n7) [below=2.5cm of n5, align=center] {JSON};

\node [below left, inner sep=3mm] at (current bounding box.north east) {\texttt{pr\_quality.py}};

\draw[->] (n1) to (n2);
\draw[<->] (n2) to (n3);
\draw[->] (n2) to (n4);
\draw[->] (n4) to (n5);
\draw[<->] (n5) to (n6);
\draw[->] (n2) edge node[sloped, below, align=center, font=\fontsize{8pt}{8pt}\selectfont] {Project\\information} (n7);
\draw[->] (n5) edge node[right, yshift=-4mm, align=center, font=\fontsize{8pt}{8pt}\selectfont] {Pull requests\\code quality} (n7);
\end{tikzpicture}
\caption{The \texttt{pr\_quality.py} workflow}\label{fig:mining_workflow}
\end{figure}
Information about the pull requests is retrieved using the \texttt{pr\_quality.py}
script. This script takes the names of the projects that will be analyzed as the
input, and it outputs the JSON files containing information about the projects
and their code quality (Figure~\ref{fig:mining_workflow}). The script
needs to retrieve the metadata for each project and its pull requests. There
are two possible sources that can be used: GitHub REST API and the GHTorrent
database. Which source will be used can be specified by passing an argument to
the tool. Metadata are then used to determine which objects need to be
fetched from the GitHub to perform the code quality analysis. The analysis of
the pull request itself is performed by an external tool called \texttt{git-contrast}.

The \texttt{gh\_db.py} is a script responsible for querying the GHTorrent database in order to
obtain data about the projects. The GHTorrent database~\cite{ghtorrent} is an offline mirror of
data offered through the GitHub REST API. \texttt{gh\_db.py} returns a JSON file
with the information about the project, such as the number of stars, number of
contributors, or information about pull requests and their commits.

An alternative script that can be used by \texttt{pr\_qality.py} is \texttt{gh\_rest.py}.
This script uses the GitHub REST API directly. The advantage of this
script is that it can retrieve the newest data from GitHub. Unfortunately,
the REST API is limited by the number of requests per hour. Because of that,
the \texttt{gh\_rest.py} is programmed to retrieve only a subset of data that are
obtained by \texttt{gh\_db.py} (data not crucial for the analysis are
omitted).

However, GitHub lacks information about the code quality of
the pull requests. This is where the \texttt{git-contrast} comes into play.
\texttt{git-contrast} is the command-line application that analyzes the code quality
of the given pull request using the external linters. This application is
further discussed in the following sections.
\section{GitHub metadata}
\label{sec:orgf27b20d}
As stated before, the scripts \texttt{gh\_db.py} and \texttt{gh\_rest.py} are used
to retrieve data from GitHub. GitHub can be leveraged to obtain
many interesting metadata, which can possibly influence the acceptance of pull
requests. All the metadata that are obtained using the scripts are listed
in Table \ref{tab:orga71f9c2}.
\begin{table}[h]
\caption{\label{tab:orga71f9c2}Data retrieved from GitHub}
\centering
\begin{tabular}{|llcc|}
\hline
Level & Metadata & \texttt{gh\_db.py} & \texttt{gh\_rest.py}\\
\hline
\hline
Repository level & Project name & \ding{51} & \ding{51}\\
 & Programming language & \ding{51} & \ding{51}\\
 & Time of creation & \ding{51} & \ding{51}\\
 & Number of forks & \ding{51} & \ding{51}\\
 & Number of commits & \ding{51} & \ding{55}\\
 & Number of project members & \ding{51} & \ding{55}\\
 & Number of stars & \ding{51} & \ding{51}\\
\hline
Submitter level & Username & \ding{51} & \ding{51}\\
 & Number of followers & \ding{51} & \ding{55}\\
 & Status in the project & \ding{51} & \ding{51}\\
\hline
Pull request level & Pull request ID & \ding{51} & \ding{51}\\
 & Is PR accepted? & \ding{51} & \ding{51}\\
 & Time opened & \ding{51} & \ding{51}\\
 & Head repository & \ding{51} & \ding{51}\\
 & Head commit & \ding{51} & \ding{51}\\
 & Base commit & \ding{51} & \ding{51}\\
 & Number of commits & \ding{51} & \ding{55}\\
 & Number of comments & \ding{51} & \ding{55}\\
\hline
\end{tabular}
\end{table}

Metadata like ``Number of stars'' or ``Time opened'' are required for the
statistical analysis.  Others are not meant to be used as a part of the
analysis itself but are kept here for better orientation, and some of them
are needed for the \texttt{git-contrast} tool, such as ``Head commit'', ``Base commit'', etc.
\section{Evaluating code quality}
\label{sec:orgd68ce38}
\texttt{git-contrast} is the command-line application that I implemented in order to
be able to analyze the code quality of the given pull request. The \texttt{git-contrast}
expects two commit hashes on the input and returns the information about the
change in code quality between these commits on the output.
The number of found code quality issues is
then written to the standard output.

To measure the change in the quality of the pull request, the
\texttt{git-contrast} is run on the ``head commit'' and the ``base commit'' of the given
pull request. The \texttt{git-contrast} supports several linters; which linter will be
used is determined by the file extension of the tested file (Table \ref{tab:org57934c1}).
\begin{table}[htbp]
\caption{\label{tab:org57934c1}Linters supported by \texttt{git-contrast}}
\centering
\begin{tabular}{|lcll|}
\hline
Linter & Version & Programming language & File extensions\\
\hline
\hline
\href{https://pylint.pycqa.org/}{\textbf{Pylint}} & 2.12.2 & Python & \texttt{.py}\\
\href{https://pmd.github.io/}{\textbf{PMD}} & 6.42.0 & Java & \texttt{.java}\\
\href{https://ktlint.github.io/}{\textbf{ktlint}} & 0.43.2 & Kotlin & \texttt{.kt} and \texttt{.kts}\\
\href{https://github.com/ndmitchell/hlint}{\textbf{HLint}} & 3.2.8 & Haskell & \texttt{.hs}\\
\href{https://dwheeler.com/flawfinder/}{\textbf{flawfinder}} & 2.0.19 & C/C++ & \texttt{.c}, \texttt{.cpp} and \texttt{.h}\\
\hline
\end{tabular}
\end{table}

The most problematic was to statically analyze the C/C++ source files because
some linters also need the information on how the source code should be
compiled. I tested the OCLint and Cppcheck linters but without success.
The compilation flags cannot always be automatically determined from the makefiles.
Because of that, I settled on using the flawfinder, which performs a simpler analysis and
does not require compilation flags.

The following linters are supported by \texttt{git-contrast}:
\begin{description}
\item[{Pylint}] Python linter that is able to detect programming errors and helps
enforce coding standards\footnote{\url{https://peps.python.org/pep-0008/}}.
Issues are divided into the following categories: conventions, code smells,
warnings (Python-specific problems), and errors.
\item[{PMD}] Linter that is able to discover common programming flaws. It is mainly
concerned with Java and Apex programming languages. PMD is extensible but also
provides many predefined rulesets: ``Best Practices'', ``Code style'', ``Security''\dots{}
All Java rule sets available in the basic installation were used to evaluate code quality.
\item[{ktlint}] Simple static analyzer focused on the code clarity and community
conventions\footnote{\url{https://kotlinlang.org/docs/coding-conventions.html}}.
This linter uses only a small set of carefully selected rules.
\item[{HLint}] Tool for suggesting possible improvements to Haskell code.
Every hint has one of the following severity levels: error, warning, and suggestion.
\item[{flawfinder}] A simple program that examines C/C++ code and searches for potentially
dangerous functions. This is done using the built-in database of functions with
well-known problems. Linter uses the following risk levels: note, warning, and error.
\end{description}
\section{Projects selection}
\label{sec:orgddcbc6e}
In total, 100 projects were selected written in five different
programming languages (20 projects for each language).  The analyzed GitHub
projects were selected based on the following criteria:
\begin{itemize}
\item The primary programming language is Python, Java, Kotlin, Haskell, or C/C++.
\item The project is popular --- it is in the top 150 most favorite projects written in the given language.
One of the reasons to analyze popular projects is the fact that popularity influences acceptance~\cite{social}.
Popular projects also usually contain a high number of pull requests.
Two different lists of popular projects were used: projects sorted by the
number of stars using the GHTorrent database (data from \nth{1} June 2019) and the list from
GitHub\footnote{\url{https://github.com/EvanLi/Github-Ranking}} (data from \nth{1} January 2022).
\item The project contains at least 200 pull requests that are suitable for analysis.
This means that PR needs to contain at least one file written in the
primary language and the data about PR needs to be publicly available.
\item The project is using GitHub to merge pull requests (for most of the pull requests).
\item The project is a library, program, or collection of programs. Repositories whose primary purpose is
to store configuration files, documentation, books, etc., were ignored.
\end{itemize}
\section{{\bfseries\sffamily TODO} Computational resources}
\label{sec:orgf656e2f}
\chapter{Data analysis}
\label{sec:org82f3c27}
In this chapter, I am explaining which statistical methods were chosen in
order to answer the research questions. RQ\textsubscript{1}--RQ\textsubscript{4} were
analyzed separately for each programming language; therefore, also the
techniques that will be discussed were applied separately.  Only the last
research question discuss multiple languages at the same time and compares
results retrieved from the individual analysis of each language.
\section{RQ\textsubscript{1}: Which code issues are typically introduced by the pull requests?}
\label{sec:org0044dcd}
At first, in order to answer the RQ\textsubscript{1}, I summarized the retrieved data for each project
--- I counted how many suitable pull requests were analyzed and
how many of them were accepted/rejected. Then I created a scatter plot between the number of
stars and the percentage of accepted PRs.

I also summarized all pull requests regardless of their project. I computed the average number
of introduced issues, fixed issues, etc. Then I created a heat map that shows how many PRs
introduced/fixed some specific number of issues.

Then for each issue individually, I computed how many accepted/rejected pull
requests introduced/fixed this issue, how many times this issue occurred in
some pull request, etc. I created multiple lists of issues sorted by various parameters.
I sorted issues by the number of rejected/accepted PRs that fixed/introduced them.
I also listed issues and the percentage of PRs that changed their quality. I examined the
issues that were fixed in a larger number of PRs than introduced. Then I created a scatter plot
that shows which issue category is the most common.

These steps were applied individually for each programming language to determine
how does the average PR look line in terms of code quality.
\section{RQ\textsubscript{2}: Are there some particular quality flaws that affect the acceptance of the pull request?}
\label{sec:org52fdb9c}
In order to discover issues that affect the acceptance of pull requests
most, the classification models were created.  The aim of these models is to
classify pull requests into two groups (accepted PRs and rejected PRs) by
using the information about the quality change in the given pull
request. Multiple classification algorithms were
used\footnote{\url{https://scikit-learn.org/stable/modules/classes.html}}:
\begin{description}
\item[{LogisticRegression~\cite{logisticreg}}] Despite its name, logistic regression
is a linear model used for classification. It uses
a so-called \emph{logistic function} that turns the inputs (code quality issues)
into the probability of the dependent variable (PR acceptance) being 1 (PR is
accepted).
\item[{DecisionTrees~\cite{dectrees}}] This algorithm constructs the tree where leaves represent
the different classes (PR accepted/rejected),
and inner nodes represent the so-called \emph{split criterion} --- the condition
(or predicate) on single/multiple attributes (code quality issues).
The \emph{split criterion} defines to which subtree given input (pull
request) belongs.
\item[{Bagging~\cite{bagging}}] The Bagging algorithm is trying to predict the data class (PR being rejected/accepted)
using multiple different classifiers. It uses bootstrapping\footnote{random sampling with replacement}
to construct the different data sets for each
classifier. The outputs from these classifiers are then aggregated to form
the final prediction.
\item[{RandomForest~\cite{randforest}}] This classifier leverages the bagging method in order to create the forest of
uncorrelated decision trees (to avoid bias and overfitting). Unlike the decision trees,
the RandomForest uses only a subset of features (code quality issues) to generate the decision tree
(this ensures the low correlation between the trees).
\item[{ExtraTrees~\cite{extratrees}}] ExtraTrees is a classifier similar to RandomForest.
The main difference is that the ExtraTrees algorithm generates \emph{split
criterions} using randomization.  Another key difference is that
ExtraTrees uses whole original sample for each tree (instead of
bootstrapping).
\item[{AdaBoost~\cite{adaboost}}] The AdaBoost is another algorithm that leverages multiple weak classifiers
(usually DecisionTrees with only one \emph{split criterion}) to predict the final result. It begins by fitting a
classifier on the original dataset. Each subsequent classifier is
improved using the results from the previous one (incorrectly classified
pull requests have a higher chance of being selected in the next
classifier).
\item[{GradientBoost~\cite{gradient}}] The GradientBoost algorithm is similar to the AdaBoost. It is also
using multiple weak classifiers, and they are trained one by one. However,
instead of improving the
subsequent classifier by changing the training dataset distribution, the GradientBoost algorithm
trains the classifiers using the residual errors of predecessors. Furthermore, the GradientBoost
works with larger trees than AdaBoost.
\item[{XGBoost~\cite{xgboost}}] XGBoost is a popular variant of gradient boosting. It is designed to be fast
and efficient. It can generate multiple tree nodes in parallel. Furthermore, \emph{regularization} is used
to prevent overfitting.
\end{description}
Each of those algorithms was run on three different datasets:
\begin{itemize}
\item a dataset with quality change
\item a dataset containing only introduced issues
\item a dataset with only fixed issues
\end{itemize}
In the first dataset, the quality change for some issues was
represented by the integer, and this integer was negative if the issue was fixed in the PR
and positive if the issue was introduced. The other datasets were created by filtering
positive/negative values from the first dataset. Running the classification algorithms on
the dataset with only fixed issues can help to understand if the improvement in code quality
can also influence the acceptance.

In order to recognize issues that have some effect on the PR acceptance,
the \emph{drop-column importance} mechanism\footnote{\url{https://explained.ai/rf-importance/}} was used.
This mechanism is resource-intense (requires a lot of computational power) but is usually more reliable
than the classic importance mechanisms.

The dataset was split into five parts to better evaluate the model accuracy
(5-fold cross-validation).  Each model was then trained five times ---
a distinct dataset was used for training and for validation.  Several metrics
(precision, recall, AUROC, F-measure\dots{}) were used to evaluate the
reliability of each model. Afterward, the average metrics over all folds
were computed.

The same technique was used by Lenarduzzi et al.~\cite{quality}. The script
they provided was used to run the classification algorithms. It was only
slightly modified to improve the user interface. Furthermore, the option
to filter only fixed/introduced issues was added.
\section{RQ\textsubscript{3}: Is there a relationship between the source code quality and the pull request acceptance?}
\label{sec:org51c1bdb}
At first, the PCA (principal component analysis) scatter plot was created to
visualize the difference between accepted and rejected pull requests.

The impact of the presence of some code issue in the PR on the PR acceptance was
determined using the \(\chi^2\) test. In order to perform this test, the dataset
was transformed into a \emph{contingency table}.  This table (\(2 \times 2\)) contained
the number of accepted/rejected PRs with/without a code quality issue.
After that, the \(\chi^2\) test of independence was performed on the
\emph{contingency table}.  The \emph{significance level} was set to \(\alpha =
   0.05\). However, relying only on statistical significance can be misleading
because it is affected by sample size. To understand the practical
significance of the test (\emph{effect size}), the Cramer's V denoted as \(\phi_c\)
was also computed. The Cramer's V ranges between 0 (no
association) and 1 (complete association).

Pull request that adds or removes some files greatly influences
code quality. If the number of removed/added files has a large impact on PR
acceptance (regardless of code quality), then it can be a large threat to
the validity of the independence test.  The pull request acceptance can also be
influenced by the quality of files which were not linted (were written in
non-primary language).  To eliminate the risk that the test was influenced,
the same test was performed on pull requests that only modified some source
files, and these files were written in the primary language.

Moreover, the \(\chi^2\) test was performed independently for each issue
category to understand if there are some issue categories that have a
stronger influence on the quality.

The test was also computed for each project separately. Unluckily, there are
some projects that contain an insufficient number of pull requests.  According
to Cochran~\cite{cochran}, all expected counts should be ten or greater.
Therefore, the tests were performed only on some projects (that have a sufficient
number of expected counts).

It is important to note that p-values were not adjusted in any way.

The metrics obtained from classification algorithms were also used to
determine if the code quality has some impact on PR acceptance.
\section{RQ\textsubscript{4}: Does code quality influence the time required to close a pull request?}
\label{sec:org32c1c64}
In order to find the possible link between the code quality and the time it
takes to close a PR, regression algorithms were used. At first, the
dataset was split into two parts --- training and test set.  After that, the
regression model was trained on the training set. Then, the importance of
individual quality issues was determined using the \emph{permutation importance}
mechanism. Afterward, the model was used to predict the
time based on the data from the test set. Metrics such as \emph{mean absolute error}
(MAE), \emph{mean squared error} (MSE), and \emph{coefficient of determination} (\(R^2\))
were computed using the predicted and expected values and used to evaluate the
models.

Following regressors were used\footnote{\url{https://scikit-learn.org/stable/modules/linear\_model.html}}:
\begin{description}
\item[{LinearRegression~\cite{linreg}}] Linear regression is a commonly used type of predictive model.
It is used for modeling the linear relationship between explanatory variables (code quality issues)
and a scalar response (time to close a PR). The model that minimizes the residual sum of squares
is selected.
\item[{ElasticNet~\cite{elasticnet}}] ElasticNet is an extension of linear regression. It is adding \(L_1\) (lasso regression)
and \(L_2\) (ridge regression) penalties in order to make the linear model more robust.
The problem with the classic linear regression is that the estimated coefficients can be
too high due to overfitting. Because of that, the model parameters are added to the
\emph{loss function}\footnote{a function that is minimized during the regression} as a penalty.
\item Some of the already discussed methods used for classification were also used for regression.
Following methods were used for both classification and regression:
\textbf{DecisionTree}, \textbf{RandomForest}, \textbf{AdaBoost}, \textbf{Bagging}, and \textbf{GradientBoost}.
\end{description}
\section{RQ\textsubscript{5}: Is code quality impact higher in projects that are using some particular programming language?}
\label{sec:org5086b72}
The RQ\textsubscript{3} discusses the impact of code quality on individual
programming languages. The findings from the RQ\textsubscript{3} for each
language are compared in the RQ\textsubscript{5}. This comparison is a complicated
task because each language has different characteristics, and
a different linter was used to measure its code quality.

The results from \(\chi^2\) tests were compared to identify
the possible difference between the languages (in terms of code
quality). The metrics retrieved from classification models were
also compared. Finally, the code quality effect on the time to close a PR
was compared between the languages (using the metrics from regressors).
\chapter{Evaluation}
\label{sec:org2c89279}
The following chapter is dedicated to the findings from my research.  The first
five subchapters focus on individual programming languages --- here I am
giving the answers to the first four research questions.  The last research
question (RQ\textsubscript{5}) is answered afterward. At the end of this chapter, I am
discussing possible threats to validity that could eventually influence the
outcomes of my study.
\section{Python}
\label{sec:orgd9ede29}
In order to analyze the influence of code quality on the pull request
acceptance, 20 projects from the Python ecosystem were selected.
In total, 9452 pull requests were analyzed, and 73 \% of these PRs were accepted.
Pull requests were more accepted in less popular projects, as can be seen in
the following scatterplot:
\tikzFigure{results/python/}{stars_and_acceptance}{Stars and pull request acceptance}{fig:python_stars}

On average, one pull request introduced 5.36 issues and fixed 2.44 issues;
an accepted pull request introduced 4.62 and fixed 1.99 issues, and rejected
pull request introduced 7.86 issues and fixed 4.43 on average.
5 \% trimmed mean was used to compute these values.
\tikzFigure{results/python/}{pr_quality_heat_map}{Pull requests and quality}{fig:python_quality}

In the analyzed pull requests, Pylint detected 222 different issues.

The conventions dominated the list of issues that were fixed/introduced in
the largest number of pull requests.  The convention that was
fixed/introduced in the largest number of pull requests is
\texttt{missing-function-docstring} (in 37 \% of PRs); conventions
\texttt{invalid-name}, \texttt{line-too-long} and \texttt{consider-using-f-string} were
fixed/introduced in over 20 \% of pull requests. There were 15 issues
that were fixed/introduced in more than 10 \% of PRs, and 72 issues were in
over 1 \% of PRs (out of the 222 issues which were found in the pull
requests).  There were nine issues that were present in the analyzed pull
requests but did not influence their quality (the number of these issues was not
changed by any pull request). 13 issues were introduced/fixed in only one
pull request, and 10 of them are issues classified as errors. The most common
error is \texttt{import-error} (24 \% of PRs); however, I suspect that there will be
many false positives that arise due to linting in the isolated
environment. Sixty issues were fixed in more PRs than they were introduced.
They are 24 more PRs that fixed the warning \texttt{super-init-not-called} than the
PRs that introduced it.
\tikzFigure{results/python/}{issues_types_and_prs}{Pylint issues and \% of PRs which fixed/introduced them}{fig:python_types}

The most important Pylint issue in regards to the PR acceptance is the
\texttt{syntax-error}.  XGBoost classifier gives this error the 1.2 \%
importance. However, other classifiers consider this error less important.
On average importance of the \texttt{syntax-error} is only 0.3 \%.  The syntax error
was introduced in 17 projects. On average, rejected pull request introduced
\texttt{0.027} syntax errors, and the average accepted pull request even fixed \texttt{0.001}
syntax errors.
\tikzFigure{results/python/}{issue_importance}{Ten most important Pylint issues}{fig:python_importance}

When only introduced issues were considered, the list
of the most important issues looked differently. On the other hand, there
are some issues that appeared in the top 10 in both lists: \texttt{syntax-error},
\texttt{unused-variable} and \texttt{unused-import}. The \texttt{syntax-error} is considered
the most important issue by both methods.

When only the information about fixed issues is used, the most important issue
is \texttt{f-string-without-interpolation} (in terms of acceptance). However, no classifier
gives this issue importance over one percent.

In order to visualize the difference in quality between accepted and rejected PRs, I created PCA scatter plot:
\tikzFigure{results/python/}{acceptance_pca}{PCA scatter plot}{fig:python_pca}
In the PCA scatter plot, there is no visible difference between rejected and accepted pull requests.

To understand if the presence of some issue in the PR influences its acceptance, I created contingency matrices
and performed a \(\chi^2\) test of independence:
\tikzFigure{results/python/}{acceptance_ct}{Relationship between presence of issue and PR acceptance}{fig:python_ct}

As can be seen in Figure~\ref{fig:python_ct}, the observed number of
rejected pull requests which contained some defected is higher than
expected. For \(\chi^2\) test, \(p < \num{2.2e-16}\) and therefore, the
hypothesis that presence of some issue and PR acceptance are independent is
rejected on significance level \(\alpha = 0.05\). However, the Cramer's \(\phi_c
   \approx 0.092\); therefore, the association between issue presence and acceptance is weak.
This conclusion also supports the fact that AUROC for trained classification models is only slightly over 0.5.
The average AUC for all models is \(0.534\).

When considering only PRs that solely modified some source files, \(p < \num{5.548e-10}\)
and therefore also here the presence of some code quality issue in the PR
influences the PR acceptance.
Similar to the previous test, the \(\phi_c \approx 0.087\); therefore, the
association between the presence of the same issue and PR acceptance is weak.

Almost identical results were obtained when the \(\chi^2\) test was performed separately for each issue category.

When the projects were considered individually, only for nine of them the \(p < \alpha\). In these projects,
the poor code quality had a negative impact on PR acceptance.
In the rest of the projects, the presence of some code quality issue does not seem to have an effect on
the PR acceptance.

The quality of the code does not seem to have an effect on the time it takes to close a pull request.
All of the trained regression models have a negative \(R^2\) score (when evaluated on the test set).
This means that trained models are worse at predicting the time than a constant (mean value).
Similar results were obtained when only introduced issues were considered and also when only
fixed issues were considered.
\section{Java}
\label{sec:orgb09340b}
The next programming language that was analyzed is Java. In total, the 8887
pull requests were linted, and 73 \% of these pull requests were accepted.
On average, the one pull request introduced 20 new PMD issues but,
at the same time, also fixed 18 other issues.

Like int the Python projects, the pull request from the less popular project
were more likely to be accepted than pull requests from more popular projects.
\tikzFigure{results/java/}{stars_and_acceptance}{Stars and pull request acceptance}{fig:java_stars}

Only 1366 pull requests (from the total of 8887 pull requests) did not change
the quality of the source code (did not fix nor introduce some PMD issues).
The PMD linter was able to detect 253 different issues in the given pull requests.
Most of the introduced issues were issues related to the code style. In total,
all of the pull requests introduced over a million code-style issues.
\tikzFigure{results/java/}{pr_quality_heat_map}{Pull requests and quality}{fig:java_quality}

The issue that was introduced in the largest number of pull requests is
\texttt{CommentRequired} (documentation issue).  Another frequent issues are
\texttt{LocalVariableCouldBeFinal}, \texttt{MethodArgumentCouldBeFinal} (code style issues)
and \texttt{LawOfDemeter} (issue in code design). These issues are the only issues
that were introduced in more than 3000 pull requests. Similarly, the list of issues
that were fixed in the largest number of the pull request is dominated by the
very same issues.

\tikzFigure{results/java/}{issues_types_and_prs}{PMD issues and \% of PRs which fixed/introduced them}{fig:java_types}
As can be seen in Figure~\ref{fig:pmd_issues_prs}, the documentation issues
tend to appear in a large number of pull requests (24 \% on average). Moreover,
the typical code style issue appeared in 11 \% of pull requests. On the other
end of the spectrum, an average issue indicating an error-prone construct is present in only two
percent of pull requests.

\tikzFigure{results/java/}{issue_importance}{Ten most important PMD issues}{fig:java_importance}
The most important PMD issue is \texttt{JUnitAssertionsShouldIncludeMessage}. The
average importance of this issue is only 0.6 \%. However, the AdaBoost
classifier gives this issue 3.7 \% importance.  The 0.89 issues of this type
are introduced in an average accepted pull request. I suspect that the pull
requests that are adding a larger number of tests to the codebase have a higher
probability of being accepted. At the same time, these pull requests also have a higher probability
of introducing the \texttt{JUnitAssertionsShouldIncludeMessage}. This can be the
reason why this issue has the largest importance.  This also supports the
study that shows that the acceptance likelihood is increased by 17.1 \% when
tests are included~\cite{social}. However, another performed study indicates that
the presence of test code does not influence PR acceptance~\cite{exploratory}.

The PCA scatter plot for Java pull requests looks as follows:
\tikzFigure{results/java/}{acceptance_pca}{PCA scatter plot}{fig:java_pca}
In the PCA scatter plot, there is no visible difference between rejected/accepted pull requests.

To understand the relationship between acceptance and the introduction of a quality issue,
the \(\chi^2\) test was performed.
\tikzFigure{results/java/}{acceptance_ct}{Relationship between presence of issue and PR acceptance}{fig:java_ct}
The \(p = \num{9.132e-14} < \alpha\) and \(\phi_c = 0.079\); therefore, there is
a weak relation between acceptance and issue presence. Similar results were
obtained when only PRs that solely modified the source code of the main language were
considered and also when the test was performed individually for each issue category.

17 out of the 20 Java projects contained a sufficient number of pull requests to
perform the \(\chi^2\) tests. In nine of them, the code quality and acceptance are
not independent. Unexpectedly, in one of the projects (\texttt{alibaba/fastjson}) the
presence of an issue has a small positive effect on the acceptance.

The PMD issues seem to have some effect on the time it takes to close a pull
request when considering only \(R^2\) computed for each model. However, the
\(R^2\) value is usually not a good metric for evaluate non-linear models;
it can reveal some information about the model, but it does not give us
information on how accurate the model is. There are three models that have \(R^2
   > 0.4\): Bagging, GradientBoost and RandomForest.  The linear regression has
\(R^2 = 0.1257\); therefore for this model, 13 \% of the variance in time to close a
PR can be explained by quality issues. However, all of the models have high mean
absolute error (MAE). The average MAE value for all of the models is \(3934338
   \approx 46\text{ days}\) and 87 \% of all analyzed Java pull requests were
closed within one month. Therefore these models are basically useless in
practice. The other models (when considering only rejected/fixed issues) yielded
similar results. To conclude, the found quality issues do not seem to have an
effect on the time to close a pull request.
\section{Kotlin}
\label{sec:org01056e8}
The 20 projects were also selected from the Kotlin ecosystem.
The average analyzed pull request was from a project that has ten thousand
stars and introduced nine issues and fixed only four. The 7514 pull requests
were analyzed (using the \emph{ktlint} linter), and 80 \% of them were accepted.
The trend that maintainers of popular projects reject more pull requests can
also be observed in the Kotlin community.
\tikzFigure{results/kotlin/}{stars_and_acceptance}{Stars and pull request acceptance}{fig:kotlin_stars}

Only 20 different issues were detected by the \emph{ktlint} in the analyzed projects;
however, this is expected since the \emph{ktlint} is focused only on a small set of quality issues.

The \texttt{indent} is the issue that was introduced in the largest number of pull requests (2598). It is the
only issue that was introduced in more than a thousand pull requests. It is also the issue
that was fixed in the largest number of pull requests. The official Kotlin convention is
to use the four spaces for indentation\footnote{\url{https://kotlinlang.org/docs/coding-conventions.html}},
and the \texttt{indent} issue signifies that this convention was violated. This issue influenced
the code quality of more than half of the pull requests. However, this can be caused by projects
whose standards do not follow the official recommendations.

Other often violated \emph{ktlint} rules are \texttt{no-wildcard-imports}, \texttt{final-newline}, and \texttt{import-ordering}.
On the other end of the spectrum, the rule \texttt{no-line-break-after-else} was violated only once.

\tikzFigure{results/kotlin/}{pr_quality_heat_map}{Pull requests and quality}{fig:kotlin_quality}

\tikzFigure{results/kotlin/}{issue_importance}{Ten most important ktlint issues}{fig:kotlin_importance}
The issue with the highest importance average is \texttt{dot-spacing}.
The Bagging classifier gives 1.7 \% importance to this issue. The importance obtained from other
classifiers is smaller --- the average importance is 0.8 \%.
However, this issue was introduced only in 18 PRs (13 times in the rejected pull request).
Furthermore, seven accepted and seven rejected pull requests fixed this issue.
Therefore the impact of this issue is disputable.

It is worth mentioning that fourth most important issue does not have a name
(given by \emph{ktlint}).  This issue usually indicates an invalid Kotlin file.  This
issue has high importance (relative to the other issues) also when the only
fixed and also when only introduced issues were taken into account during the
classification. This issue was introduced by 90 rejected PRs and by 51
accepted PRs.

When using only introduced issues, the most important issue is \texttt{indent}.
This issue is also most important when only the fixed issues are considered.
As being said before, in projects that are using non-standard indentation,
this issue is a false positive.

The PCA scatter plot was also created for the Kotlin programming language.
The first principal component explains almost all variance in the code quality of pull requests.
However, the difference between rejected/accepted pull requests is not apparent from the PCA plot:
\tikzFigure{results/kotlin/}{acceptance_pca}{PCA scatter plot}{fig:kotlin_pca}

To understand the link between acceptance and the introduction of some
quality issue, I performed the \(\chi^2\) test on Kotlin dataset. The \(p < \num{2.2e-16}\) and
\(\phi_c \approx 0.095\); therefore, the presence of some issue has a small
negative effect on acceptance (similarly to the Java and Python).
Furthermore, three classifiers (\emph{Bagging}, \emph{GradientBoost}, and \emph{RandomForest})
have AUC for ROC curve above 60, and the average AUC is \(57.58\).
\tikzFigure{results/kotlin/}{acceptance_ct}{Relationship between presence of issue and PR acceptance}{fig:kotlin_ct}
However, taking into account solely the PRs that only modified some source
code, the \(p = 0.627\), thus the acceptance and issue presence are independent
(in this context).

Only 12 of the projects have a sufficient number of pull requests to evaluate
the \(\chi^2\) test. There are four projects where the presence of some issue
has a small impact on the PR acceptance (the average Cramer's V is \(\phi_c = 0.18\)).

To analyze the relation between the code quality and time that is required to
close a PR, I applied several regression techniques also to the Kotlin
dataset.  For linear regression, \(R^2 = 0.164\), therefore the trained model is
able to explain 16 \% of the variance in the time to close a PR. The \(MAE =
   2375121 \approx 27\text{ days}\); therefore, the model does not perform so well
on the dataset, taking into consideration that 89 \% of pull requests were
closed within one month. The mean absolute error for other models was similar
to the \(MAE\) obtained for linear regression.
\section{Haskell}
\label{sec:org4e80d24}
Haskell is the only purely functional programming language that was analyzed.
The 18 out of 20 selected Haskell projects have under the 5000 stars. There
are only two exceptions: PureScript with 7632 stars and Pandoc, which has over
15000 stars. The Pandoc has the also smallest percentage of accepted pull
requests.  However, excluding the Pandoc, there is no visible connection
between the number of stars and acceptance in the selected projects. When the
outliers are filtered, the trend tends to be the opposite of previous languages:
more accepted are pull requests of projects with more stars.  However, only
20 projects are not sufficient to make such conclusions about the whole
population of Haskell projects.
\tikzFigure{results/haskell/}{stars_and_acceptance}{Stars and pull request acceptance}{fig:haskell_stars}

The 6949 pull requests were analyzed. Interestingly, in over 60
\% of pull requests, no change in the code quality was detected. Moreover, the
\emph{HLint} is able to recognize a large number of different issues (321 issue
types were detected in selected pull requests). On the other hand, some issues
were counted twice because they appeared as a suggestion but also as a warning
(in the different contexts).
These facts can indicate that a large
number of submitted pull requests follow high-quality standards.
\tikzFigure{results/haskell/}{pr_quality_heat_map}{Pull requests and quality}{fig:haskell_quality}

Seventy-eight percent of pull requests were accepted, and the average pull request introduced
only 0.6 issues and fixed 0.3 issues. The most common types of issues were suggestions
and warnings. The error that was introduced in the largest number of pull requests is
\texttt{Use-newTVarIO}, and this error was introduced only in 8 pull requests. The most common
suggestions were \texttt{Redundant-bracket} (introduced in 499 PRs) and \texttt{Redundant-\$} (444 PRs).
The warning \texttt{Unused-LANGUAGE-pragma} was introduced in 323 pull requests and \texttt{Eta-reduce}
warning in 214 of them. There were only ten issues that were introduced in 100 and more
pull requests; and another 105 issue types were detected in the analyzed code, but no PR introduced
any of those issues.
\tikzFigure{results/haskell/}{issues_types_and_prs}{HLint issues and \% of PRs which fixed/introduced them}{fig:haskell_types}

The most important Haskell issue is the suggestion \texttt{Use-if}. However, no classifier gives this
issue importance over one percent. Therefore the actual impact of this issue is disputable.
This issue was introduced in 18 rejected PRs and fixed in 11. There are 19 accepted PRs that
introduced \texttt{Use-if} and 27 accepted PRs that fixed it.
\tikzFigure{results/haskell/}{issue_importance}{Ten most important HLint issues}{fig:haskell_importance}
When only introduced issues were taken into account, the most important issue is \texttt{Move-brackets-to-avoid-\$} (suggestion).
The AdaBoost classifier gives this issue 1 \% importance, although the average importance is only 0.4 \%.

In the context of fixed issues, the most important is warning \texttt{Use-fewer-imports} with average importance again only about 0.4 \%.

The PCA scatter plot was also generated for the Haskell language.
Similar to the results in already analyzed languages, there is no apparent
difference between accepted and rejected pull requests.
\tikzFigure{results/haskell/}{acceptance_pca}{PCA scatter plot}{fig:haskell_pca}

For the \(\chi^2\) test, the \(p = 0.001438 < \alpha = 0.05\) and Cramer's V is only \(\phi_c = 0.038\);
therefore, the presence of an issue in the PRs has only a small negative impact on the
acceptance of the pull request. Similar results were obtained when only
the pull requests that contain exclusively some modified code were considered.
Furthermore, tests for the individual issue types also yielded similar results.
Unfortunately, there is only a small number of pull requests that introduced some errors;
therefore the \(\chi^2\) test cannot be performed on this issue category.
The average AUC computed for ROC curves is around 50 --- the classification algorithms
were unable to distinguish between the accepted and rejected PRs using the code quality.
\tikzFigure{results/haskell/}{acceptance_ct}{Relationship between presence of issue and PR acceptance}{fig:haskell_ct}
The 13 projects contain a sufficient number of pull requests; the acceptance and
the issue presence are not independent only in four of them (there, the issue presence
have a small negative impact on the acceptance). For the \texttt{haskell/aeson} project,
the Cramer's V is \(0.282\) --- the association is ``medium''.

The issues detected by \emph{HLint} do not seem to have an impact on the time it takes to close a pull request.
All trained models have negative \(R^2\). When only fixed issues were used for regression, there
were three models with positive \(R^2\): Bagging (0.0315), ElasticNet (0.0085), and RandomForest (0.0229).
However, all of them have high mean absolute error: Bagging (\(2193658 \approx 25\text{ days}\)),
ElasticNet (2255678), and RandomForest (2201347).
\section{C/C++}
\label{sec:org8d0cf47}
The C and C++ programming languages are analyzed together because they share
a lot of similarities.  This usually enables use of the same linter for both
languages. Moreover, it is not uncommon that projects that are written in C++
also contain some C code and vice versa.  The nine selected projects have more
code written in C, while the rest of the 11 projects is more C++-oriented.

In analyzed projects, there is no visible connection between the acceptance
and the number of stars.
\tikzFigure{results/c_cpp/}{stars_and_acceptance}{Stars and pull request acceptance}{fig:c_cpp_stars}

I analyzed 8774 C/C++ pull requests. Seventy-seven percent of them have been accepted.
The typical pull request introduces 0.25 issues and fixes 0.12 issues; the typical
rejected PR introduces 0.79 issues, and the typical accepted PR only 0.15 issues.
The 79 \% of pull requests did not change the quality of the source code
(in terms of the \emph{flawfinder} quality rules).
\tikzFigure{results/c_cpp/}{pr_quality_heat_map}{Pull requests and quality}{fig:c_cpp_quality}

The most common type of issue is the note. The least common are errors. The
\emph{flawfinder} was able to identify 137 different issues in the studied
PRs. All of the top ten issues (in terms of number of PRs which introduced
them) are notes. The most common note is \texttt{buffer-char} (``Statically-sized
arrays can be improperly restricted leading to potential overflows or other
issues\dots{}''). The most common error is \texttt{buffer-strcat} (``Does not check
for buffer overflows when concatenating to destination\dots{}''), and it is the
11 most introduced issue (introduced in 69 pull requests). There are 36 issues
that were present in the analyzed code, but they were not introduced in any
pull request; 21 of them are errors.
\tikzFigure{results/c_cpp/}{issues_types_and_prs}{flawfinder issues and \% of PRs which fixed/introduced them}{fig:c_cpp_types}

Classification algorithms rank as the most important issue the
\texttt{format-printf} (``If format strings can be influenced by an attacker, they
can be exploited\dots{}''). However, this issue is only a \emph{note}.  Therefore it
does not have to indicate a defect (there will probably be a large number of
false positives). AdaBoost and XGBoost algorithms give this issue importance
of 1 \%. The average importance is 0.7\%.  This issue is also most important
when only introduced issues are considered.  The second most important issue has
average importance of only 0.26 \%.

The most important error is \texttt{buffer-StrCpyNA} (``Does not check for buffer
overflows when copying to destination\dots{}'') with average importance of only
0.9 \%. This error is the sixth most important issue.
\tikzFigure{results/c_cpp/}{issue_importance}{Ten most important flawfinder issues}{fig:c_cpp_importance}
When considering only fixed issues, the \texttt{buffer-read} is the most important issue (note);
however, the average importance is only 0.28 \%.

The PCA analysis does not reveal any significant difference between the accepted
and rejected pull requests (in terms of code quality).
\tikzFigure{results/c_cpp/}{acceptance_pca}{PCA scatter plot}{fig:c_cpp_pca}

Based on the \(\chi^2\) test, the presence of an issue in the PR has a small negative impact
on the PR acceptance (\(\phi_c = 0.117\)).
However, When considering only pull requests that solely modified some source files,
Cramer's V \(\phi_c = 0.024\) and \(p = 0.1 > \alpha\) --- in this settings, the issue presence
does not influence acceptance.
\tikzFigure{results/c_cpp/}{acceptance_ct}{Relationship between presence of issue and PR acceptance}{fig:c_cpp_ct}
Some small impact impact was discovered when the \(\chi^2\) test was performed separately for
each issue category (the \(p < \num{2.2e-16}\) and \(\phi_c \approx 0.1\) for each category).
Furthermore, in 6 out of 11 projects which have enough data to perform and evaluate the \(\chi^2\) test,
the presence of some issue in the PR has a negative effect on the PR acceptance.
In the \texttt{minetest/minetest} and \texttt{pybind/pybind11} projects, this effect is moderate;
for other projects, the association is small.

In the case of C/C++, the time to close a pull request seems not to be related
to found issues.  All the models have negative \(R^2\), except the ElasticNet
regressor. For the ElasticNet, \(MAE = 4681624\) (the mean absolute error is 54
days) --- therefore, this model also cannot be used to predict the time to
close a PR.  Models considering only rejected issues and also models
considering only accepted issues have yielded similar results.
\section{Programming languages and code quality impact}
\label{sec:orga86995c}
Comparing the code quality of projects written in different programming
languages is a difficult task.  Each language has different programming
constructs, syntax, and type system. For instance, Python, which is
a dynamically-typed multi-paradigm programming language, has completely
distinct characteristics from Haskell, which is a purely functional programming
language with a strong, static type system.

Moreover, every linter is different and has a unique set of rules.  The
\emph{ktlint} is focused on code clarity and community conventions, whereas
\emph{flawfinder} checks code for potentially dangerous functions. On the other
hand, the \emph{PMD} is a more general-oriented linter that contains a large set of
rules for the Java programming language. Lastly, the \emph{HLint} is oriented mainly
on code simplification and spotting redundancies.

On the other hand, there are some metrics that evaluate how effectively
trained models predict the acceptance of PR or time to close a PR;
and these metrics can be compared across different programming languages.
On top of that, the results from the \(\chi^2\) test can also be compared.
However, the cation is in order because the code quality for each language is
evaluated differently, as discussed before.
\tikzFigure{results/}{all_cramers_v}{Comparison of Cramer's V}{fig:all_v}
As can be seen in Figure~\ref{fig:all_v}, in all studied languages, the
presence of some issue have a negative effect on the PR acceptance (in terms
of \(\chi^2\) tests); however, for all of the languages, this effect is small (\(\phi_c
   \approx 0.1\)).  The smallest effect was observed for Haskell programming
language and the highest effect for C/C++.  On the other hand,
taking into account solely the PRs that only modified some source code of the
primary language, the \(\chi^2\) test indicates that the presence of issue and PR
acceptance are independent in the case of the C/C++ and Kotlin. This is a possible
threat to validity.

The effect of code quality on acceptance was also studied using
classification algorithms.  One of the metrics that were used to measure the
performance of the classification models is the ``area under the ROC curve''
(AUC). When using this metric to evaluate models, the Haskell is once again
the language when the code quality is least important
(Figure~\ref{fig:all_auc}). The average AUC for Haskell models is around
0.5 --- the trained models are no better than random guessing.
The models for the Kotlin are ranked with the highest AUC score and therefore
are better in classification than models for other languages.
Except for Haskell, the average AUC is over 0.5 but under 0.6 --- these AUC
scores are usually considered poor~\cite{logreg}. This indicates that code quality
has only a small or no effect on the acceptance.

As can be seen, similar results were obtained for all of the languages.
In all of the languages, the code quality impact is small (based on the \(\chi^2\) tests
and also based on the results from classification algorithms). There is no language
that significantly differs from others.
\tikzFigure{results/}{all_auc}{AUC for differenct languages (ROC)}{fig:all_auc}

As discussed in the previous chapter, there seems to be no connection between
the code quality and the time it takes to close pull requests (based on the
trained regression models).  The smallest MAE was scored by Kotlin models
(around 26 days); on the other end of the spectrum are Python models with an
average MAE equal to 78.9 days. The trained models are unusable, considering
that most of the pull requests are closed within the first two weeks (83 \% of Kotlin
PRs and 76\% of Python PRs).
\tikzFigure{results/}{all_mae}{Mean absolute error for prediction of time to close a PR}{fig:all_mae}
\section{Threats to validity}
\label{sec:orgeaa0498}
The validity of my research is endangered by several things.
At first, the selection of the projects is one of the factors that influence
the outcomes of the research. This study is focused primarily on popular
projects. The rationale behind the project selection is explained in the own dedicated
subchapter. It is possible that projects selected using different metrics can yield
varying results.

Another possible threat to validity is the selection of pull requests.
It is usually not doable to examine all the pull requests of some project.
For the projects with a huge number of PRs, the time and computational resources are
the limiting factors. Moreover, to examine the rejected pull requests, the forked
repository with the required commits needs to be available. This is not always the case.
Sometimes the \emph{force push} can also remove the commits from the accepted pull requests.
It is also important to note that linting of some pull requests resulted in an error in the
linter, and therefore these PRs were skipped. Pull requests were also skipped if the
linting time exceeded the limit (that was set to 1000 seconds) --- the PRs that
modified a huge number of files were ignored. Furthermore, for some projects,
the number of analyzed pull requests was limited to 500 to reduce the total
time required for analysis.

Another problem is that pull requests can be merged manually outside GitHub.
These pull requests are not recognized as accepted~\cite{ghperils}. The projects were selected so that
GitHub is the primary way to merge PRs. However, there still can be some PRs
merged using alternative methods.

Furthermore, different methods can be used to measure the quality of pull
requests. For each programming language, there exist several linters that
are focused on a different set of issues, and they can also use different algorithms
to detect the same issue. Another possible threat are false positives from linters.
The false-positive can arise due to the fact that the files were linted in the
isolated environment, and this can introduce some issues (\texttt{import-error}, etc.).
Some issues are also hard to detect; for instance, the issue can be specific to
some particular context, and the linter does not have to take this context into
account. The greatest difficulty with the quality evaluation is the fact that
everyone has a unique personal perspective on code quality --- code quality
does not have a single definition.

The pull requests sometimes contain also files that are not written in the
primary programming language of the project. The pull request then can be
rejected because of these files.

Lastly, there are several factors that influence PR acceptance.
Some of them were discussed in previous chapters (number of commits, submitter's status, etc.).
The one factor that influences the acceptance is a number of lines that were changed~\cite{social}.
The more lines are added/changed, the higher the probability that the pull request will be
rejected, but the chance that some quality issue will be introduced is also higher.
In this case, it is difficult to distinguish if the pull request was rejected because
of the code quality or because the changes are too big.
\chapter{Conclusion}
\label{sec:orge3d917e}
I analyzed 41576 pull requests from 100 projects written in 5 different
programming languages (Python, Java, Kotlin, Haskell, C/C++) to study the
relationship between the code quality and pull request acceptance.  The
quality of the individual pull requests was measured using static code
analysis.

Almost half of the analyzed PRs introduced some code quality issue, and 31 \%
of pull requests fixed some issue. However, data differs significantly between
the languages (because different static analysis tools were used).  In C/C++
projects, only 16 \% of pull requests introduced some issue, while in Java,
almost 76 \% of PRs. The proportion of accepted pull requests was different for
each project; however, on average, 76 \% of PRs were accepted.

Several statistic techniques were used to understand if the code quality
affects PR acceptance. For each language, the \(\chi^2\) test of independence
was performed, and the number of accepted PRs without a code quality issue was always
higher than expected. However, in all languages, the impact on acceptance was
only small (\(\phi_c \approx 0.1\)).

Multiple classification algorithms were used to predict the pull request acceptance
using the code quality. However, all of them performed poorly (\(AUROC < 0.6\)).
The most problematic was the Haskell language --- all models were no better than a random
predictor (\(AUROC \approx 0.5\)).

The trained models were also used to understand the importance of individual issues.
Unfortunately, no issue with a significant impact on the PR acceptance was detected.
All discovered issues have average importance below 1 \% (between all models).

The influence of code quality on time to close a PR was also
studied. Several regression models were trained to predict this time. However,
all of the trained models have very high \emph{mean absolute error}: around one month.
This makes models worthless because most of the PRs are closed within two
weeks.

To conclude, the poor code quality seems to have a small negative impact on the
pull request acceptance. However, there seems to be no effect on the time it
takes to close a PR.
\section{Comparison with related work}
\label{sec:org0ba47a0}
Best of my knowledge, there is only one study~\cite{quality} about the effect of quality flaws
on the pull request acceptance. Lenarduzzi et al. analyzed 28 well-known Java projects.
I reused the script they provided for PR classification and also applied similar statistical
techniques so that my findings could be compared with theirs.
The \(\chi^2\) test of independence yielded similar results (they obtained \(\phi_c = 0.12\)).
My classification models have slightly better performance (mean \(AUROC\) is higher by \(0.023\)).
The difference in performance can be caused by various factors --- project
selection and the ratio of accepted pull requests (only 53 \% of PRs they studied were accepted).
The code quality was evaluated using the same linter (PMD). However, I also took into
account issues that were fixed by the PR. Moreover, a different technique was used to identify
issues that were introduced in the PR (they used diff-files provided by GitHub API).
Similar to my findings, Lenarduzzi et al. did not identify any particular issues that have a significant
effect on the acceptance.

I extended the work of Lenarduzzi with an analysis of four new programming
languages (Python, Kotlin, Haskell, and C/C++).  I also added the analysis
of the delivery time of pull requests, and as far as I know, this is the first
study that researches the relationship between the code quality and the time it
takes to close a pull request.
\section{Future work}
\label{sec:org0f9e898}
I consider my work complete. However, there is still plenty of
possibilities for how to improve and expand my work.
Several improvements can be made to obtain more reliable data for analysis.
If the pull request is not merged using GitHub, then the PR is incorrectly classified as rejected.
It is possible to utilize some heuristics that will recognize merging through plain Git utilities.

For the proper quality evaluation, the linter choice is essential. Each
linter is focused on some specific set of issues, and this can introduce
some form of bias. It would be beneficial to use multiple linters for one
programming language. The linters used for C/C++ and Kotlin are not very
sophisticated.  However, adding a more advanced linter for C/C++ is
complicated --- the state-of-the-art linters require information about
compiler flags. This information cannot always be retrieved automatically
(from makefiles).  Therefore, a lot of pull requests require manual
customization.

Some projects use linters as part of the \emph{continuous integration} or
during the build process. Additional research needs to be performed to
understand if the maintainers of those projects are more strict about the
code quality, and therefore the effect on the PR acceptance is larger.
\chapter*{Appendix}
\label{sec:org0fa219a}
\addcontentsline{toc}{chapter}{Appendix}
\section*{Scripts used for analysis}
\label{sec:orgf9558d8}
\begin{itemize}
\item \textbf{TODO:} fix grammar
\end{itemize}
In order to simplify analysis of retrieved data, I created the script (\texttt{pr\_process.py}) that
takes multiple JSON files with the data about each individual project and
converts them into the CSV files. Each row in the CSV file represents some
pull request. This script also filters the pull requests which are not
suitable for the analysis --- PRs that do not contains any source code written
in the primary language or PRs that contained corrupted files (the linter was
unable to analyze those files).

The retrieved data about the pull request were subsequently analyzed in order
to answer my research questions. For the classification (RQ\textsubscript{2}) was used the Python
script\footnote{\url{https://figshare.com/s/d47b6f238b5c92430dd7}} (\texttt{pr\_classification.py}) provided by Lenurdazzi
et al.~\cite{quality}.

I also created the script (\texttt{pr\_reqression.py}) that runs the regression algorithms on the data in order to answer RQ\textsubscript{4}.
This script is written in Python and it uses scikit-learn\footnote{\url{https://scikit-learn.org/stable/index.html}} library.

The rest of the analysis was done using the \texttt{analysis.R}. This small R program imports the data
generated by other scripts. This data are then analyzed using various statistical methods.
Script is also used to plot graphs, create tables and then export them directly into the \LaTeX{}.
\section*{Projects}
\label{sec:org62ed643}
\begin{table}[H]
\caption{Python projects}
\centering
\begingroup\scriptsize
\input{results/python/projects_summary}
\endgroup
\end{table}

\begin{table}[H]
\caption{Java projects}
\centering
\begingroup\scriptsize
\input{results/java/projects_summary}
\endgroup
\end{table}

\begin{table}[H]
\caption{Kotlin projects}
\centering
\begingroup\scriptsize
\input{results/kotlin/projects_summary}
\endgroup
\end{table}

\begin{table}[H]
\caption{Haskell projects}
\centering
\begingroup\scriptsize
\input{results/haskell/projects_summary}
\endgroup
\end{table}

\begin{table}[H]
\caption{C/C++ projects}
\centering
\begingroup\scriptsize
\input{results/c_cpp/projects_summary}
\endgroup
\end{table}

\FloatBarrier
\section*{Issue categories}
\label{sec:orgc29f3c1}
\begin{table}[H]
\caption{Pylint issue categories}
\centering
\begingroup\scriptsize
\input{results/python/issue_types_summary}
\endgroup
\end{table}

\begin{table}[H]
\caption{PMD issue categories}
\centering
\begingroup\scriptsize
\input{results/java/issue_types_summary}
\endgroup
\end{table}

\begin{table}[H]
\caption{HLint issue categories}
\centering
\begingroup\scriptsize
\input{results/haskell/issue_types_summary}
\endgroup
\end{table}

\begin{table}[H]
\caption{flawfinder issue categories}
\centering
\begingroup\scriptsize
\input{results/c_cpp/issue_types_summary}
\endgroup
\end{table}

\FloatBarrier
\section*{Classification metrics}
\label{sec:orgdb5a3d8}
\begin{table}[H]
\caption{Python classification metrics}
\centering
\begingroup\scriptsize
\input{results/python/classification_metrics}
\endgroup
\end{table}

\begin{table}[H]
\caption{Java classification metrics}
\centering
\begingroup\scriptsize
\input{results/java/classification_metrics}
\endgroup
\end{table}

\begin{table}[H]
\caption{Kotlin classification metrics}
\centering
\begingroup\scriptsize
\input{results/kotlin/classification_metrics}
\endgroup
\end{table}

\begin{table}[H]
\caption{Haskell classification metrics}
\centering
\begingroup\scriptsize
\input{results/haskell/classification_metrics}
\endgroup
\end{table}

\begin{table}[H]
\caption{C/C++ classification metrics}
\centering
\begingroup\scriptsize
\input{results/c_cpp/classification_metrics}
\endgroup
\end{table}

\FloatBarrier
\subsection*{ROC curves}
\label{sec:orgddb65be}
\tikzFigure[H]{results/python/}{roc_curves}{Python classification ROC}{fig:python_roc}
\tikzFigure[H]{results/java/}{roc_curves}{Java classification ROC}{fig:java_roc}
\tikzFigure[H]{results/kotlin/}{roc_curves}{Kotlin classification ROC}{fig:kotlin_roc}
\tikzFigure[H]{results/haskell/}{roc_curves}{Haskell classification ROC}{fig:haskell_roc}
\tikzFigure[H]{results/c_cpp/}{roc_curves}{C/C++ classification ROC}{fig:c_cpp_roc}

\FloatBarrier
\section*{Regression metrics}
\label{sec:orgb26bc2c}
\begin{table}[H]
\caption{Python regression metrics}
\centering
\begingroup\scriptsize
\input{results/python/regression_metrics}
\endgroup
\end{table}

\begin{table}[H]
\caption{Java regression metrics}
\centering
\begingroup\scriptsize
\input{results/java/regression_metrics}
\endgroup
\end{table}

\begin{table}[H]
\caption{Kotlin regression metrics}
\centering
\begingroup\scriptsize
\input{results/kotlin/regression_metrics}
\endgroup
\end{table}

\begin{table}[H]
\caption{Haskell regression metrics}
\centering
\begingroup\scriptsize
\input{results/haskell/regression_metrics}
\endgroup
\end{table}

\begin{table}[H]
\caption{C/C++ regression metrics}
\centering
\begingroup\scriptsize
\input{results/c_cpp/regression_metrics}
\endgroup
\end{table}

\FloatBarrier
\subsection*{Absolute error density}
\label{sec:org9c3d95e}
\tikzFigure[H]{results/python/}{regression_absolute_error}{Python AE density}{fig:python_ae}
\tikzFigure[H]{results/java/}{regression_absolute_error}{Java AE density}{fig:java_ae}
\tikzFigure[H]{results/kotlin/}{regression_absolute_error}{Kotlin AE density}{fig:kotlin_ae}
\tikzFigure[H]{results/haskell/}{regression_absolute_error}{Haskell AE density}{fig:haskell_ae}
\tikzFigure[H]{results/c_cpp/}{regression_absolute_error}{C/C++ AE density}{fig:c_cpp_ae}

\FloatBarrier
\section*{{\bfseries\sffamily TODO} Create table that compares already performed studies with my thesis}
\label{sec:orge03d71c}
\begin{itemize}
\item Replication Can Improve Prior Results: A GitHub Study of Pull Request Acceptance~\cite{replication}
\begin{itemize}
\item contains interesting table with factors that influences acceptance
\end{itemize}
\item Pull Request Decision Explained: An Empirical Overview~\cite{empirical}
\begin{itemize}
\item also contains interesting table with factors that influences acceptance
\end{itemize}
\end{itemize}
\end{document}